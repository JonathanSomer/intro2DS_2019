{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS Regression - continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A statistical model for inference \n",
    "\n",
    "### So far we did not assume any specific *true* relationship between $y$ and $x$\n",
    "\n",
    "### Let us now *assume* the following model: \n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\epsilon,\\;\\;\\epsilon \\sim N(0,\\sigma^2),\\;\\;\\;\\;\\Rightarrow\\;\\; y|x \\sim N (x^T\\beta, \\sigma^2).$$\n",
    "1. $E(y|x) = x^T\\beta$ is a linear function of $x$\n",
    "2. The error $(y-E(y|x))$ has a normal distribution and is independent for each observation\n",
    "\n",
    "### If this assumption holds, we can investigate the distribution of $\\hat{\\beta}$ and use that to do inference on the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distribution of the OLS solution under the model assumptions:  \n",
    "\n",
    "### What we know: \n",
    "$$  (a)\\; E(Y) = X\\beta,\\;\\;\\;\\; (b)\\; Cov(Y) = \\sigma^2 I_n ,\\;\\;\\;\\;(c)\\; \\hat{\\beta} = (X^TX)^{-1} X^T Y$$\n",
    "\n",
    "### Mean: \n",
    "$$E(\\hat{\\beta}) \\stackrel{(c)}{=} (X^TX)^{-1} X^T E(Y) \\stackrel{(a)}{=} (X^TX)^{-1} X^T X\\beta = \\beta.$$\n",
    "\n",
    "### Covariance matrix: \n",
    "$$ Cov(\\hat{\\beta}) \\stackrel{(c)}{=} (X^TX)^{-1} X^T Cov(Y) X (X^TX)^{-1} \\stackrel{(b)}{=} \\sigma^2 (X^TX)^{-1} (X^T X) (X^TX)^{-1} = \\sigma^2 (X^TX)^{-1}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical inference\n",
    "\n",
    "### From the previous formulas we conclude: $ \\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 (X^TX)^{-1}_{j,j}).$\n",
    "\n",
    "### Recall that our second goal (beyond prediction) was *inference*: which variables are important?  \n",
    "\n",
    "### Now we can formalize this as a hypothesis test: for each variable $j$, test the null $H_{0j}: \\beta_j = 0.$\n",
    "\n",
    "### If $H_{0j}$ holds, then $ \\hat{\\beta}_j \\sim N(0, \\sigma^2 (X^TX)^{-1}_{j,j}).$\n",
    "\n",
    "### Assuming $\\sigma^2$ is known, this leads to a simple $Z$-test as we studied\n",
    "\n",
    "### Since $\\sigma^2$ is not known, we need to estimate it and get a $T$-test instead (details omitted). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to the 14-movies model, now with the inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.276\n",
      "Model:                            OLS   Adj. R-squared:                  0.275\n",
      "Method:                 Least Squares   F-statistic:                     217.7\n",
      "Date:                Tue, 30 Apr 2019   Prob (F-statistic):               0.00\n",
      "Time:                        09:49:26   Log-Likelihood:                -9690.2\n",
      "No. Observations:                8000   AIC:                         1.941e+04\n",
      "Df Residuals:                    7985   BIC:                         1.952e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================================================\n",
      "                                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "constant                                                   0.4096      0.090      4.562      0.000       0.234       0.586\n",
      "Independence Day                                           0.0607      0.013      4.581      0.000       0.035       0.087\n",
      "The Patriot                                               -0.0187      0.012     -1.608      0.108      -0.041       0.004\n",
      "The Day After Tomorrow                                     0.0344      0.011      3.160      0.002       0.013       0.056\n",
      "Pirates of the Caribbean: The Curse of the Black Pearl     0.0610      0.012      5.104      0.000       0.038       0.084\n",
      "Pretty Woman                                               0.1578      0.012     13.657      0.000       0.135       0.180\n",
      "Forrest Gump                                              -0.0621      0.013     -4.664      0.000      -0.088      -0.036\n",
      "The Green Mile                                             0.0281      0.013      2.087      0.037       0.002       0.055\n",
      "Con Air                                                    0.0659      0.012      5.405      0.000       0.042       0.090\n",
      "Twister                                                    0.0988      0.012      8.358      0.000       0.076       0.122\n",
      "Sweet Home Alabama                                         0.2304      0.011     21.171      0.000       0.209       0.252\n",
      "Pearl Harbor                                               0.0401      0.010      3.839      0.000       0.020       0.061\n",
      "Armageddon                                                 0.0135      0.013      1.054      0.292      -0.012       0.039\n",
      "The Rock                                                  -0.0085      0.013     -0.662      0.508      -0.034       0.017\n",
      "What Women Want                                            0.1406      0.011     12.258      0.000       0.118       0.163\n",
      "==============================================================================\n",
      "Omnibus:                      143.151   Durbin-Watson:                   2.026\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.798\n",
      "Skew:                          -0.320   Prob(JB):                     1.09e-33\n",
      "Kurtosis:                       3.214   Cond. No.                         150.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results14.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS regression summary\n",
    "\n",
    "### Minimize RSS on $Tr$ to find the \"best\" linear fit for $Y$ as a function of $X$\n",
    "\n",
    "### Algebraic solution, geometric interpretation: projection\n",
    "\n",
    "### Under the assumed statistical model (strong assumptions!) can do inference on which variables are important\n",
    "\n",
    "### The most important tool in the statistical/predictive modeling toolbox!\n",
    "\n",
    "### Learn more: Statistical Models course in Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS interpretation: estimating conditional expectation\n",
    "\n",
    "### As we just saw, under the statistical model, $E\\hat{\\beta} = \\beta \\;\\Rightarrow\\; E(\\hat{y}|x) = x^T E (\\hat{\\beta}) =  x^T \\beta = E(y|x).$  \n",
    "\n",
    "### Even when the model doesn't hold, the use of RSS / squared error loss implies estimation of conditional expectation (details omitted)\n",
    "\n",
    "### Hence an interpretation of the OLS prediction is an *attempt* to estimate the conditional expectation $E(y|x)$\n",
    "\n",
    "### This conditional expectation is clearly interesting: it summarizes what we learned about $y$ from seeing $x$\n",
    "\n",
    "### The attempt may not be successful, if the model is not so good (more on that later), but at least we know what we are trying to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about classification?\n",
    "\n",
    "### We will focus on the simplest (and most important) case of two-class classification: \n",
    "* Girl vs boy\n",
    "* Sick vs healthy\n",
    "* Buy vs don't buy\n",
    "\n",
    "### As before, we have $Tr = (X,Y)$ of size $n$, $Te$ of size $m$. \n",
    "\n",
    "### For now, keep assuming $x \\in \\mathbb R^p$ is numeric as in the eBay shirts example\n",
    "\n",
    "### Can we use the OLS mechanism we have built to build a classification model? \n",
    "\n",
    "### For sure we can, if we encode $y=girl\\Rightarrow y=1,\\;\\;y=boy\\Rightarrow y=0$, we have numeric $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is wrong with using OLS for classification? \n",
    "\n",
    "### If we encode $y$ as above what is $E(y|x)$? It is $P(y=girl|\\;picture)$ --- a clearly interesting quantity\n",
    "\n",
    "### Problem: as a probability, $0\\leq P(y=girl|\\;picture) \\leq 1.$ But model predictions $x^T\\hat{\\beta}$ can fall outside the legal range!\n",
    "\n",
    "### Another problem: can we make the model assumptions of normal $\\epsilon$? No --- because $y$ can only be $0$ or $1$\n",
    "\n",
    "### The idea: try to create an approach that is similar to OLS, but more fitting for classification, taking into account the limited range of values and the need for a sensible statistical model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression\n",
    "\n",
    "### Deals with the two problems above\n",
    "\n",
    "### We start from assuming a model: \n",
    "$$ \\log\\frac{P(y=1|x)}{P(y=0|x)} = x^T\\beta$$\n",
    "\n",
    "### Notice that now all values are legal: \n",
    "$$ 0\\leq P(y=1|x) \\leq 1 \\;\\; \\Leftrightarrow\\;\\; -\\infty \\leq \\log\\frac{P(y=1|x)}{P(y=0|x)} \\leq \\infty.$$\n",
    "\n",
    "### Another way of writing this: \n",
    "$$ P(y=1|x) = \\frac{\\exp(x^T\\beta)}{1+\\exp(x^T\\beta)} $$\n",
    "$$ P(y=0|x) = 1- P(y=1|x) = \\frac{1}{1+\\exp(x^T\\beta)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a logistic regression\n",
    "\n",
    "### Given training data $Tr$, we want to find the best coefficients $\\hat{\\beta}.$\n",
    "\n",
    "### This is done by maximum likelihood, finding $\\beta$ to maximize: \n",
    "$$ \\max_\\beta \\prod_{i=1}^n  \\left(\\frac{\\exp(x_i^T\\beta)}{1+\\exp(x_i^T\\beta)}\\right)^{y_i} \\left(\\frac{1}{1+\\exp(x_i^T\\beta)}\\right)^{1-y_i}$$\n",
    "\n",
    "### The solution is $\\hat{\\beta}$, the logistic regression coefficients estimates\n",
    "\n",
    "### Predicting on $x \\in Te$: \n",
    "$$ \\widehat{P(y=1|x)} = \\frac{\\exp(x^T\\hat{\\beta})}{1+\\exp(x^T\\hat{\\beta})}\\;\\; \\Rightarrow\\;\\; \\hat{y} = \\left\\{ \\begin{array}{ll} 1 & \\mbox{if} \\widehat{P(y=1|x)}> 0.5 \\\\\n",
    "0 & \\mbox{otherwise}\\end{array}\\right. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation of coefficients\n",
    "\n",
    "### We can write our model as: \n",
    "$$\\log\\frac{P(y=1|x)}{P(y=0|x)} = x^T\\beta$$\n",
    "\n",
    "### The expression on the left is called the *log odds*: log of the ratio of positive vs negative probability\n",
    "\n",
    "### Interpretation: ${\\beta}_j$ is the change in the log odds from a change of 1 unit in $x_j$. \n",
    "\n",
    "### For example, if ${\\beta}_j=1$ then when $x_j=1$ vs $x_j=0$ the log odds increase by $1$, so the odds increase times $e=2.72$, which is roughly the increase in ${P(y=1|x)}$ when it is close to $0$. \n",
    "\n",
    "### When estimating from $Tr$: add hats over all quantities and remember these are only estimates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "saheart = pd.read_table(\"http://statweb.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", header = 0,sep=',',index_col=0)\n",
    "saheart.head()\n",
    "\n",
    "saheart_X=pd.get_dummies(saheart.iloc[:,:9]).iloc[:,:9]\n",
    "saheart_y=saheart.iloc[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>138.326840</td>\n",
       "      <td>3.635649</td>\n",
       "      <td>4.740325</td>\n",
       "      <td>25.406732</td>\n",
       "      <td>53.103896</td>\n",
       "      <td>26.044113</td>\n",
       "      <td>17.044394</td>\n",
       "      <td>42.816017</td>\n",
       "      <td>0.346320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.496317</td>\n",
       "      <td>4.593024</td>\n",
       "      <td>2.070909</td>\n",
       "      <td>7.780699</td>\n",
       "      <td>9.817534</td>\n",
       "      <td>4.213680</td>\n",
       "      <td>24.481059</td>\n",
       "      <td>14.608956</td>\n",
       "      <td>0.476313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>3.282500</td>\n",
       "      <td>19.775000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>22.985000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>134.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.340000</td>\n",
       "      <td>26.115000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>25.805000</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>148.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.790000</td>\n",
       "      <td>31.227500</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>28.497500</td>\n",
       "      <td>23.892500</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>218.000000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>42.490000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>46.580000</td>\n",
       "      <td>147.190000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sbp     tobacco         ldl   adiposity       typea     obesity  \\\n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000   \n",
       "mean   138.326840    3.635649    4.740325   25.406732   53.103896   26.044113   \n",
       "std     20.496317    4.593024    2.070909    7.780699    9.817534    4.213680   \n",
       "min    101.000000    0.000000    0.980000    6.740000   13.000000   14.700000   \n",
       "25%    124.000000    0.052500    3.282500   19.775000   47.000000   22.985000   \n",
       "50%    134.000000    2.000000    4.340000   26.115000   53.000000   25.805000   \n",
       "75%    148.000000    5.500000    5.790000   31.227500   60.000000   28.497500   \n",
       "max    218.000000   31.200000   15.330000   42.490000   78.000000   46.580000   \n",
       "\n",
       "          alcohol         age         chd  \n",
       "count  462.000000  462.000000  462.000000  \n",
       "mean    17.044394   42.816017    0.346320  \n",
       "std     24.481059   14.608956    0.476313  \n",
       "min      0.000000   15.000000    0.000000  \n",
       "25%      0.510000   31.000000    0.000000  \n",
       "50%      7.510000   45.000000    0.000000  \n",
       "75%     23.892500   55.000000    1.000000  \n",
       "max    147.190000   64.000000    1.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saheart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 (369, 9) (93, 9) (369,) (93,)\n"
     ]
    }
   ],
   "source": [
    "n = saheart_X.shape[0]\n",
    "tr_size = math.floor(0.8*n)\n",
    "te_size = n-tr_size\n",
    "tr_ind = random.sample(range(n),tr_size)\n",
    "Xtr = saheart_X.iloc[tr_ind,:]\n",
    "Xte = saheart_X.drop(saheart_X.index[tr_ind])\n",
    "Ytr = saheart_y.iloc[tr_ind]\n",
    "Yte = saheart_y.drop(saheart_y.index[tr_ind])\n",
    "\n",
    "print(n,Xtr.shape, Xte.shape, Ytr.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The predictive modeling way: using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-5.8570881]\n",
      "coef: [[ 0.00261356  0.09891859  0.17304114  0.01014803  0.03958216 -0.03098118\n",
      "  -0.00087939  0.05461037 -0.79788522]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod = LogisticRegression(solver='lbfgs',max_iter=10000)\n",
    "mod.fit (Xtr, Ytr)\n",
    "print('intercept:', mod.intercept_)\n",
    "print('coef:',mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The statistics modeling way: using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.502271\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    chd   No. Observations:                  369\n",
      "Model:                          Logit   Df Residuals:                      359\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Tue, 07 May 2019   Pseudo R-squ.:                  0.2219\n",
      "Time:                        09:49:25   Log-Likelihood:                -185.34\n",
      "converged:                       True   LL-Null:                       -238.19\n",
      "                                        LLR p-value:                 1.101e-18\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -5.8241      1.478     -3.941      0.000      -8.721      -2.927\n",
      "sbp                0.0027      0.006      0.412      0.681      -0.010       0.015\n",
      "tobacco            0.0995      0.031      3.216      0.001       0.039       0.160\n",
      "ldl                0.1738      0.071      2.464      0.014       0.036       0.312\n",
      "adiposity          0.0100      0.033      0.305      0.760      -0.054       0.075\n",
      "typea              0.0396      0.014      2.815      0.005       0.012       0.067\n",
      "obesity           -0.0315      0.049     -0.638      0.524      -0.128       0.065\n",
      "alcohol           -0.0010      0.005     -0.193      0.847      -0.011       0.009\n",
      "age                0.0545      0.014      3.899      0.000       0.027       0.082\n",
      "famhist_Absent    -0.8507      0.257     -3.310      0.001      -1.354      -0.347\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(Ytr, sm.add_constant(Xtr))\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50 11]\n",
      " [16 16]]\n",
      "Accuracy: 0.71 Misclassification loss: 0.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "phat = result.predict(sm.add_constant(Xte))\n",
    "yhat = phat>0.5\n",
    "conf =confusion_matrix(Yte, yhat) \n",
    "print(conf)\n",
    "print('Accuracy:',round((conf[0,0]+conf[1,1])/te_size,3), 'Misclassification loss:', round((conf[0,1]+conf[1,0])/te_size,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification models performance evaluation on test set: different measures\n",
    "\n",
    "### Different errors have different costs/value. \n",
    "\n",
    "### A way to capture this: summarize performance in different ways that capture different types of errors:\n",
    "\n",
    "#### $P = \\sum_{i=n+1}^{n+m} y_i$ number of positive examples, similarly $N$.\n",
    "\n",
    "#### $\\hat{P} = \\sum_{i=n+1}^{n+m} \\hat{y}_i$ number of positive predictions, similarly $\\hat{N}$.\n",
    "\n",
    "#### $TP = \\sum_{i=n+1}^{n+m} y_i \\hat{y}_i$ number of true positives, $FP = \\hat{P}-TP$\n",
    "\n",
    "#### $TN = \\sum_{i=n+1}^{n+m} (1-y_i) (1-\\hat{y}_i)$ number of true negatives, $FN = \\hat{N}-TN$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Prediction error: $\\;(FN+FP)/m$\n",
    "\n",
    "### Precision (positive predictive value): $\\;TP/\\hat{P}$\n",
    "\n",
    "### Recall (sensitivity, true positive rate):  $\\;TP/P$\n",
    "\n",
    "### False positive rate: $\\;FP/N$\n",
    "\n",
    "### Harmonic mean of precision and recall: $\\;F_1 = 2* \\frac{Precision \\times Recall}{Precision + Recall}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79        61\n",
      "           1       0.59      0.50      0.54        32\n",
      "\n",
      "   micro avg       0.71      0.71      0.71        93\n",
      "   macro avg       0.68      0.66      0.66        93\n",
      "weighted avg       0.70      0.71      0.70        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Yte, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification model evaluation: different goals\n",
    "\n",
    "### We can think of several different prediction goals, all potentially important: \n",
    "1. Classify correctly --- make few (weighted) errors on test set or new prediction points\n",
    "2. Predict probabilities well: $$ \\widehat{P(y=1|x)} \\approx P(y=1|x)$$ for new points\n",
    "3. Rank well: given multiple prediction points, predict which one is *more likely* to have $y=1$. \n",
    "\n",
    "\n",
    "### These different tasks can reflect in the loss function / model evaluation task:\n",
    "1. Correct classification: misclassification loss as above, also precision, recall etc.\n",
    "2. Good probability prediction: using Bernoulli loss / cross entropy: \n",
    "$$ L(y,\\hat{p}) = \\hat{p}^y (1-\\hat{p})^{(1-y)}.$$\n",
    "3. How do we measure ranking perofrmance of a model on a test set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The ROC Curve and the Area Under Curve (AUC)\n",
    "\n",
    "### The idea: to evaluate ranking performance, do not set the threshold $0.5$ but check what happens at all possible thresholds: \n",
    "1. True positive rate: what % of the positive observations pass the threshold?\n",
    "2. False positive rate: what % of the negative observations pass the threshold?\n",
    "\n",
    "### The ROC curve plots TPR vs FPR for all possible threholds: if the model ranks well, for high thresholds we will have $FPR\\approx 0$, while for low thresholds we will have $TPR \\approx 1$. \n",
    "\n",
    "### Note that even if $\\widehat{P(y=1|x)}$ predicts probabilities badly, or even if the predictions are not in the range $[0,1]$, the ranking can still be good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(Yte, phat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve for our logistic model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The area under the curve (AUC)\n",
    "\n",
    "### *For a random ranking:* $FPR \\approx TPR$ at every threshold, so we are around the diagonal $x=y$: $$AUC\\approx 0.5.$$\n",
    "\n",
    "### *For a perfect ranking model:* at high thresholds, $FPR=0$, at low thresholds $TPR=1$, hence: $$AUC=1.$$\n",
    "\n",
    "### Very nice interpretation of AUC: Assume the test set has $m_1$ ones ($y=1$) and $m_0$ zeros, then AUC is the % of correctly ranked pairs with different response: \n",
    "$$ AUC = \\frac{ \\#\\left\\{(i,j): y_i = 0, y_j=1 \\mbox{ and } \\hat{p}_i < \\hat{p}_j\\right\\}}{m_1\\times m_0}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Global vs local modeling\n",
    "\n",
    "### So far we have learned two predictive modeling techniques: OLS regression and logistic regression \n",
    "\n",
    "### Common theme: Global, parametric models (+ probabilistic model for inference) --- lots of assumptions!\n",
    "\n",
    "### A different approach: *Local* modeling: I am similar to my neighbors\n",
    "\n",
    "### Simple example: 1-nearest neighbor:\n",
    "\n",
    "1. Define a distance over the $\\cal X$ space. For $x\\in \\mathbbb R^p$ can simply choose the Euclidean distance: \n",
    "$$d(x,u) = \\|x-u\\|^2.$$\n",
    "2. For a prediction point (say $x_0 \\in Te$), find its nearest neighbor in the Tr\n",
    "$$ i_0 = \\arg\\min_i d(x_0,x_i).$$\n",
    "3. Predict $x_0$ as the response at the nearest neighbor $\\hat{y}_0 = y_{i_0}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-nearest neighbor (KNN) methods\n",
    "\n",
    "### Repeat the same steps, but instead of finding the nearest neighbor only, find the $k$ nearest points in $Tr$ to $x_0$. Assume their indexes are $i_{01},...,i_{0k}.$\n",
    "\n",
    "\n",
    "### For regression predict the average: \n",
    "$$\\hat{y}_0 = \\frac{1}{k} \\sum_{j=1}^k y_{i_{0j}}.$$\n",
    "\n",
    "### For classification predict the majority: \n",
    "$$\\hat{y}_0 = \\left\\{ \\begin{array}{ll} 1 & \\mbox{if} \\frac{1}{k} \\sum_{j=1}^k y_{i_{0j}} > 1/2  \\\\\n",
    "0 & \\mbox{otherwise}\\end{array} \\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The problem with KNN? \n",
    "\n",
    "### If the data are \"sparse\" in the space, nearest neighbors are far and the results can be very bad\n",
    "\n",
    "### *Curse of dimensionality*: if the dimension $p$ is high,  data are by definition sparse\n",
    "\n",
    "### KNN fails in these settings\n",
    "\n",
    "### Interesting solution: Adaptive local methods (*trees!*) => next week\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
