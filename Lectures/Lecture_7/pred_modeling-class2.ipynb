{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS Regression - continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A statistical model for inference \n",
    "\n",
    "### So far we did not assume any specific *true* relationship between $y$ and $x$\n",
    "\n",
    "### Let us now *assume* the following model: \n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\epsilon,\\;\\;\\epsilon \\sim N(0,\\sigma^2).$$\n",
    "1. $E(y|x) = x^T\\beta$ is a linear function of $x$\n",
    "2. The error $(y-E(y|x))$ has a normal distribution and is independent for each observation\n",
    "\n",
    "### If this assumption holds, we can investigate the distribution of $\\hat{\\beta}$ and use that to do inference on the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distribution of the OLS solution under the model assumptions:  \n",
    "\n",
    "### What we know: \n",
    "$$  (a)\\; E(Y) = X\\beta,\\;\\;\\;\\; (b)\\; Cov(Y) = \\sigma^2 I_n ,\\;\\;\\;\\;(c)\\; \\hat{\\beta} = (X^TX)^{-1} X^T Y$$\n",
    "\n",
    "### Mean: \n",
    "$$E(\\hat{\\beta}) \\stackrel{(c)}{=} (X^TX)^{-1} X^T E(Y) \\stackrel{(a)}{=} (X^TX)^{-1} X^T X\\beta = \\beta.$$\n",
    "\n",
    "### Covariance matrix: \n",
    "$$ Cov(\\hat{\\beta}) \\stackrel{(c)}{=} (X^TX)^{-1} X^T Cov(Y) X (X^TX)^{-1} \\stackrel{(b)}{=} \\sigma^2 (X^TX)^{-1} (X^T X) (X^TX)^{-1} = \\sigma^2 (X^TX)^{-1}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical inference\n",
    "\n",
    "### From the previous formulas we conclude: $ \\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 (X^TX)^{-1}_{j,j}).$\n",
    "\n",
    "### Recall that our second goal (beyond prediction) was *inference*: which variables are important?  \n",
    "\n",
    "### Now we can formalize this as a hypothesis test: for each variable $j$, test the null $H_{0j}: \\beta_j = 0.$\n",
    "\n",
    "### If $H_{0j}$ holds, then $ \\hat{\\beta}_j \\sim N(0, \\sigma^2 (X^TX)^{-1}_{j,j}).$\n",
    "\n",
    "### Assuming $\\sigma^2$ is known, this leads to a simple $Z$-test as we studied\n",
    "\n",
    "### Since $\\sigma^2$ is not known, we need to estimate it and get a $T$-test instead (details omitted). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to the 14-movies model, now with the inference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.276\n",
      "Model:                            OLS   Adj. R-squared:                  0.275\n",
      "Method:                 Least Squares   F-statistic:                     217.7\n",
      "Date:                Tue, 30 Apr 2019   Prob (F-statistic):               0.00\n",
      "Time:                        09:49:26   Log-Likelihood:                -9690.2\n",
      "No. Observations:                8000   AIC:                         1.941e+04\n",
      "Df Residuals:                    7985   BIC:                         1.952e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================================================\n",
      "                                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "constant                                                   0.4096      0.090      4.562      0.000       0.234       0.586\n",
      "Independence Day                                           0.0607      0.013      4.581      0.000       0.035       0.087\n",
      "The Patriot                                               -0.0187      0.012     -1.608      0.108      -0.041       0.004\n",
      "The Day After Tomorrow                                     0.0344      0.011      3.160      0.002       0.013       0.056\n",
      "Pirates of the Caribbean: The Curse of the Black Pearl     0.0610      0.012      5.104      0.000       0.038       0.084\n",
      "Pretty Woman                                               0.1578      0.012     13.657      0.000       0.135       0.180\n",
      "Forrest Gump                                              -0.0621      0.013     -4.664      0.000      -0.088      -0.036\n",
      "The Green Mile                                             0.0281      0.013      2.087      0.037       0.002       0.055\n",
      "Con Air                                                    0.0659      0.012      5.405      0.000       0.042       0.090\n",
      "Twister                                                    0.0988      0.012      8.358      0.000       0.076       0.122\n",
      "Sweet Home Alabama                                         0.2304      0.011     21.171      0.000       0.209       0.252\n",
      "Pearl Harbor                                               0.0401      0.010      3.839      0.000       0.020       0.061\n",
      "Armageddon                                                 0.0135      0.013      1.054      0.292      -0.012       0.039\n",
      "The Rock                                                  -0.0085      0.013     -0.662      0.508      -0.034       0.017\n",
      "What Women Want                                            0.1406      0.011     12.258      0.000       0.118       0.163\n",
      "==============================================================================\n",
      "Omnibus:                      143.151   Durbin-Watson:                   2.026\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.798\n",
      "Skew:                          -0.320   Prob(JB):                     1.09e-33\n",
      "Kurtosis:                       3.214   Cond. No.                         150.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results14.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS regression summary\n",
    "\n",
    "### Minimize RSS on $Tr$ to find the \"best\" linear fit for $Y$ as a function of $X$\n",
    "\n",
    "### Algebraic solution, geometric interpretation: projection\n",
    "\n",
    "### Under the assumed statistical model (strong assumptions!) can do inference on which variables are important\n",
    "\n",
    "### The most important tool in the statistical/predictive modeling toolbox!\n",
    "\n",
    "### Learn more: Statistical Models course in Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# OLS interpretation: estimating conditional expectation\n",
    "\n",
    "### As we just saw, under the statistical model, $E\\hat{\\beta} = \\beta \\;\\Rightarrow\\; E(\\hat{y}|x) = x^T E (\\hat{\\beta}) =  x^T \\beta = E(y|x).$  \n",
    "\n",
    "### Even when the model doesn't hold, the use of RSS / squared error loss implies estimation of conditional expectation (details omitted)\n",
    "\n",
    "### Hence an interpretation of the OLS prediction is an *attempt* to estimate the conditional expectation $E(y|x)$\n",
    "\n",
    "### This conditional expectation is clearly interesting: it summarizes what we learned about $y$ from seeing $x$\n",
    "\n",
    "### The attempt may not be successful, if the model is not so good (more on that later), but at least we know what we are trying to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about classification?\n",
    "\n",
    "### We will focus on the simplest (and most important) case of two-class classification: \n",
    "* Girl vs boy\n",
    "* Sick vs healthy\n",
    "* Buy vs don't buy\n",
    "\n",
    "### As before, we have $Tr = (X,Y)$ of size $n$, $Te$ of size $m$. \n",
    "\n",
    "### For now, keep assuming $x \\in \\mathbb R^p$ is numeric as in the eBay shirts example\n",
    "\n",
    "### Can we use the OLS mechanism we have built to build a classification model? \n",
    "\n",
    "### For sure we can, if we encode $y=girl\\Rightarrow y=1,\\;\\;y=boy\\Rightarrow y=0$, we have numeric $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is wrong with using OLS for classification? \n",
    "\n",
    "### If we encode $y$ as above what is $E(y|x)$? It is $P(y=girl|\\;picture)$ --- a clearly interesting quantity\n",
    "\n",
    "### Problem: as a probability, $0\\leq P(y=girl|\\;picture) \\leq 1.$ But model predictions $x^T\\hat{\\beta}$ can fall outside the legal range!\n",
    "\n",
    "### Another problem: can we make the model assumptions of normal $\\epsilon$? No --- because $y$ can only be $0$ or $1$\n",
    "\n",
    "### The idea: try to create an approach that is similar to OLS, but more fitting for classification, taking into account the limited range of values and the need for a sensible statistical model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic regression\n",
    "\n",
    "### Deals with the two problems above\n",
    "\n",
    "### We start from assuming a model: \n",
    "$$ \\log\\frac{P(y=1|x)}{P(y=0|x)} = x^T\\beta$$\n",
    "\n",
    "### Notice that now all values are legal: \n",
    "$$ 0\\leq P(y=1|x) \\leq 1 \\;\\; \\Leftrightarrow\\;\\; -\\infty \\leq \\log\\frac{P(y=1|x)}{P(y=0|x)} \\leq \\infty.$$\n",
    "\n",
    "### Another way of writing this: \n",
    "$$ P(y=1|x) = \\frac{\\exp(x^T\\beta)}{1+\\exp(x^T\\beta)} $$\n",
    "$$ P(y=0|x) = 1- P(y=1|x) = \\frac{1}{1+\\exp(x^T\\beta)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a logistic regression\n",
    "\n",
    "### Given training data $Tr$, we want to find the best coefficients $\\hat{\\beta}.$\n",
    "\n",
    "### This is done by maximum likelihood, finding $\\beta$ to maximize: \n",
    "$$ \\max_\\beta \\prod_{i=1}^n  \\left(\\frac{\\exp(x_i^T\\beta)}{1+\\exp(x_i^T\\beta)}\\right)^{y_i} \\left(\\frac{1}{1+\\exp(x_i^T\\beta)}\\right)^{1-y_i}$$\n",
    "\n",
    "### The solution is $\\hat{\\beta}$, the logistic regression coefficients estimates\n",
    "\n",
    "### Predicting on $x \\in Te$: \n",
    "$$ \\widehat{P(y=1|x)} = \\frac{\\exp(x^T\\hat{\\beta})}{1+\\exp(x^T\\hat{\\beta})}\\;\\; \\Rightarrow\\;\\; \\hat{y} = \\left\\{ \\begin{array}{ll} 1 & \\mbox{if} \\widehat{P(y=1|x)}> 0.5 \\\\\n",
    "0 & \\mbox{otherwise}\\end{array}\\right. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation of coefficients\n",
    "\n",
    "### We can write our results as: $$ \\widehat{\\left(\\log\\frac{P(y=1|x)}{P(y=0|x)}\\right)} = x^T\\hat{\\beta}$$\n",
    "\n",
    "### The expression on the right is called the *log odds*: log of the ratio of positive vs negative probability\n",
    "\n",
    "### Interpretation: $\\hat{\\beta}_j$ is the change in the log odds from a change of 1 unit in $x_j$. \n",
    "\n",
    "### For example, if $\\hat{\\beta}_j=1$ then when $x_j=1$ vs $x_j=0$ the log odds increase by $1$, so the odds increase times $e=2.72$, which is roughly the increase in $\\widehat{P(y=1|x)}$ when it is close to $0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "saheart = pd.read_table(\"http://statweb.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", header = 0,sep=',',index_col=0)\n",
    "saheart.head()\n",
    "\n",
    "saheart_X=pd.get_dummies(saheart.iloc[:,:9]).iloc[:,:9]\n",
    "saheart_y=saheart.iloc[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>462.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>138.326840</td>\n",
       "      <td>3.635649</td>\n",
       "      <td>4.740325</td>\n",
       "      <td>25.406732</td>\n",
       "      <td>53.103896</td>\n",
       "      <td>26.044113</td>\n",
       "      <td>17.044394</td>\n",
       "      <td>42.816017</td>\n",
       "      <td>0.346320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.496317</td>\n",
       "      <td>4.593024</td>\n",
       "      <td>2.070909</td>\n",
       "      <td>7.780699</td>\n",
       "      <td>9.817534</td>\n",
       "      <td>4.213680</td>\n",
       "      <td>24.481059</td>\n",
       "      <td>14.608956</td>\n",
       "      <td>0.476313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>3.282500</td>\n",
       "      <td>19.775000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>22.985000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>134.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.340000</td>\n",
       "      <td>26.115000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>25.805000</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>148.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5.790000</td>\n",
       "      <td>31.227500</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>28.497500</td>\n",
       "      <td>23.892500</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>218.000000</td>\n",
       "      <td>31.200000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>42.490000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>46.580000</td>\n",
       "      <td>147.190000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sbp     tobacco         ldl   adiposity       typea     obesity  \\\n",
       "count  462.000000  462.000000  462.000000  462.000000  462.000000  462.000000   \n",
       "mean   138.326840    3.635649    4.740325   25.406732   53.103896   26.044113   \n",
       "std     20.496317    4.593024    2.070909    7.780699    9.817534    4.213680   \n",
       "min    101.000000    0.000000    0.980000    6.740000   13.000000   14.700000   \n",
       "25%    124.000000    0.052500    3.282500   19.775000   47.000000   22.985000   \n",
       "50%    134.000000    2.000000    4.340000   26.115000   53.000000   25.805000   \n",
       "75%    148.000000    5.500000    5.790000   31.227500   60.000000   28.497500   \n",
       "max    218.000000   31.200000   15.330000   42.490000   78.000000   46.580000   \n",
       "\n",
       "          alcohol         age         chd  \n",
       "count  462.000000  462.000000  462.000000  \n",
       "mean    17.044394   42.816017    0.346320  \n",
       "std     24.481059   14.608956    0.476313  \n",
       "min      0.000000   15.000000    0.000000  \n",
       "25%      0.510000   31.000000    0.000000  \n",
       "50%      7.510000   45.000000    0.000000  \n",
       "75%     23.892500   55.000000    1.000000  \n",
       "max    147.190000   64.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saheart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 (369, 9) (93, 9) (369,) (93,)\n"
     ]
    }
   ],
   "source": [
    "n = saheart_X.shape[0]\n",
    "n\n",
    "tr_size = math.floor(0.8*n)\n",
    "te_size = n-tr_size\n",
    "tr_ind = random.sample(range(n),tr_size)\n",
    "Xtr = saheart_X.iloc[tr_ind,:]\n",
    "Xte = saheart_X.drop(saheart_X.index[tr_ind])\n",
    "Ytr = saheart_y.iloc[tr_ind]\n",
    "Yte = saheart_y.drop(saheart_y.index[tr_ind])\n",
    "\n",
    "print(n,Xtr.shape, Xte.shape, Ytr.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-5.60452753]\n",
      "coef: [[ 0.00921118  0.0900208   0.16986875  0.01144861  0.03372704 -0.04562691\n",
      "  -0.0040574   0.04741747 -0.96720184]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod = LogisticRegression(solver='lbfgs',max_iter=10000)\n",
    "mod.fit (Xtr, Ytr)\n",
    "print('intercept:', mod.intercept_)\n",
    "print('coef:',mod.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.505042\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    chd   No. Observations:                  369\n",
      "Model:                          Logit   Df Residuals:                      359\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sun, 05 May 2019   Pseudo R-squ.:                  0.2196\n",
      "Time:                        17:02:46   Log-Likelihood:                -186.36\n",
      "converged:                       True   LL-Null:                       -238.81\n",
      "                                        LLR p-value:                 1.593e-18\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -5.5591      1.474     -3.771      0.000      -8.448      -2.670\n",
      "sbp                0.0093      0.006      1.448      0.147      -0.003       0.022\n",
      "tobacco            0.0911      0.030      3.030      0.002       0.032       0.150\n",
      "ldl                0.1704      0.068      2.511      0.012       0.037       0.303\n",
      "adiposity          0.0117      0.032      0.368      0.713      -0.050       0.074\n",
      "typea              0.0338      0.014      2.482      0.013       0.007       0.061\n",
      "obesity           -0.0467      0.049     -0.947      0.344      -0.143       0.050\n",
      "alcohol           -0.0042      0.005     -0.765      0.444      -0.015       0.007\n",
      "age                0.0471      0.013      3.514      0.000       0.021       0.073\n",
      "famhist_Absent    -1.0319      0.259     -3.988      0.000      -1.539      -0.525\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(Ytr, sm.add_constant(Xtr))\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55,  7],\n",
       "       [16, 15]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "yhat = result.predict(sm.add_constant(Xte))>0.5\n",
    "confusion_matrix(Yte, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification models performance evaluation on test set: different measures\n",
    "\n",
    "### Recall we mentioned that different errors have different costs/value\n",
    "\n",
    "### A way to capture this: summarize performance in different ways that capture different types of errors:\n",
    "\n",
    "### $P = \\sum_{i=n+1}^{n+m} y_i$ number of positive examples, similarly $N$.\n",
    "\n",
    "### $\\hat{P} = \\sum_{i=n+1}^{n+m} \\hat{y}_i$ number of positive predictions, similarly $\\hat{N}$.\n",
    "\n",
    "### $TP = \\sum_{i=n+1}^{n+m} y_i \\hat{y}_i$ number of true positives, $FP = \\hat{P}-TP$\n",
    "\n",
    "### $TN = \\sum_{i=n+1}^{n+m} (1-y_i) (1-\\hat{y}_i)$ number of true negatives, $FN = \\hat{N}-TN$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Prediction error: $\\;(FN+FP)/m$\n",
    "\n",
    "### Precision (positive predictive value): $\\;TP/\\hat{P}$\n",
    "\n",
    "### Recall:  $\\;TP/P$\n",
    "\n",
    "### Harmonic mean of precision and recall: $\\;F_1 = 2* \\frac{Precision \\times Recall}{Precision + Recall}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.83        62\n",
      "           1       0.68      0.48      0.57        31\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        93\n",
      "   macro avg       0.73      0.69      0.70        93\n",
      "weighted avg       0.74      0.75      0.74        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Yte, yhat))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
