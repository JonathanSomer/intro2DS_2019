{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Scraping websites with BeautifulSoup\n\nThis is a basic walkthrough of harvesting data from websites that have infromation spread across multiple pages and inside links.\n\nWe'll be scraping the database of food recall warnings from the [Canadian Food Inspection Agency](http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221).\n\nFirst, the set-up. This tutorial uses Python 2.7 and three modules that need to be installed:\n\n* [requests](http://docs.python-requests.org/en/latest/) for HTTP\n* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for parsing HTML\n\n\n## Mind your ethics\n\nScraping websites carry a few ethical questions: does the site's terms of use forbid scraping? Is it copyright infringement to republish the data? Are you punishing the servers with too many requests?\n\nweb scraping is in somewhat of a gray moral area. on one side you are harvesting data that is not private and is available to you through the webpage, but you are creating trafic on the websites server, without compensating the owner by complying with the sites buisiness model (allowing them to present ads?)\nso please keep these points in mind and don't be surprised if the site owner will try to block your scraper :)\n\n## Study the site structure\n\nFirst, we need to spend time on the site to understand how the website is built. What's the URL structure? What's the general hirarchical structure of the webpage?\n\nBy poking around a bit and selecting a year in the top menu, we see that the site lists all warnings for that year. Good, less pagination work for us.\n\nThat year is also specified in the '`ay=`' parameter in the URL. And if you change that parameter to another year, all the warnins for that year are loaded.\n\n<img src=\"img/url.png\">\n\nThis will help us cycle through the different years.\n\nSo let's set up our first task, which is cycling through the years."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\n\n# Concatenate the URL up to 'ay=' with the year we want, then the tail end\nurl_head = \"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=\"\nurl_tail = \"&fr=0&fc=0&fd=0&ft=1\"\n\ndef cycle_years(start_year, end_year):\n\n    for year in range(start_year, end_year + 1):\n        r = requests.get(url_head + str(year) + url_tail)          ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's leave this for now and come back to it later. \n\nWe'll first look at how BeautifulSoup works at parsing a DOM. Let's load just the index page for 2015 recalls and explore it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "r = requests.get(\"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=2015&fr=0&fc=0&fd=0&ft=1\")\nsoup = BeautifulSoup(r.content, \"lxml\")",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# The variable soup is a BeautifulSoup object containing the entire HTML document.\n\n# Use prettify() to pretty-print the document\nprint(soup.prettify()[:1000])",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<!DOCTYPE html>\n<!--[if lt IE 9]><html class=\"no-js lt-ie9\" lang=\"en\" dir=\"ltr\"><![endif]-->\n<!--[if gt IE 8]><!-->\n<html class=\"no-js\" dir=\"ltr\" lang=\"en\">\n <!--<![endif]-->\n <head>\n  <meta charset=\"utf-8\"/>\n  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n  <title>\n   Complete listing of all recalls and allergy alerts  - Canadian Food Inspection Agency\n  </title>\n  <meta content=\"Complete listing of all recalls and allergy alerts\" name=\"dcterms.title\"/>\n  <meta content=\"Canadian Food Inspection Agency\" name=\"description\"/>\n  <meta content=\"Canadian Food Inspection Agency\" name=\"dcterms.description\"/>\n  <meta content=\"Canadian Food Inspection Agency\" name=\"keywords\"/>\n  <meta content=\"inspection\" name=\"dcterms.subject\" title=\"gccore\"/>\n  <meta content=\"Government of Canada,Canadian Food Inspection Agency\" name=\"dcterms.creator\"/>\n  <meta content=\"eng\" name=\"dcterms.language\" title=\"ISO639-2\"/>\n  <meta content=\"2012-10-29 10:06:27\" name=\"dcterms.issued\" title=\"W3\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# we can access some elements directly (for example page title)\n\nsoup.title",
      "execution_count": 32,
      "outputs": [
        {
          "data": {
            "text/plain": "<title>Complete listing of all recalls and allergy alerts  - About the Canadian Food Inspection Agency - Canadian Food Inspection Agency</title>"
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# We can keep going deeper, and really target the node we want. \n\nsoup.head.link",
      "execution_count": 55,
      "outputs": [
        {
          "data": {
            "text/plain": "<link href=\"/DAM/PresentationService/WET/4.0.19/theme-gcwu-fegc/assets/favicon.ico\" rel=\"icon\" type=\"image/x-icon\"/>"
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's say we want to get all the hyperlinks into a list:\n\nsoup.find_all(\"a\")[:20]",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "[<a class=\"wb-sl\" href=\"#wb-cont\">Skip to main content</a>,\n <a class=\"wb-sl\" href=\"#wb-info\">Skip to \"About this site\"</a>,\n <a class=\"wb-sl\" href=\"#wb-sec\">Skip to section menu</a>,\n <a href=\"https://www.canada.ca/en.html\" rel=\"external\">Canada.ca</a>,\n <a href=\"https://www.canada.ca/en/services.html\" rel=\"external\">Services</a>,\n <a href=\"https://www.canada.ca/en/government/dept.html\" rel=\"external\">Departments</a>,\n <a href=\"/au-sujet-de-l-acia/salle-de-nouvelles/avis-de-rappel-d-aliments/liste-complete/fra/1351519587174/1351519588221?ay=2015&amp;fr=0&amp;fc=0&amp;fd=0&amp;ft=1\" lang=\"fr\">Fran√ßais</a>,\n <a aria-controls=\"mb-pnl\" class=\"overlay-lnk btn btn-sm btn-default\" href=\"#mb-pnl\" role=\"button\" title=\"Search and menus\"><span class=\"glyphicon glyphicon-search\"><span class=\"glyphicon glyphicon-th-list\"><span class=\"wb-inv\">Search and menus</span></span></span></a>,\n <a href=\"/eng/1297964599443/1297965645317\">Canadian Food Inspection Agency</a>,\n <a class=\"item\" href=\"#aboutthecfia\">About the <abbr title=\"Canadian Food Inspection Agency\">CFIA</abbr></a>,\n <a href=\"/about-the-cfia/acts-and-regulations/eng/1299846777345/1299847442232\">Acts and Regulations</a>,\n <a href=\"/about-the-cfia/contact-the-cfia/eng/1546627816321/1546627838025\">Contact Us</a>,\n <a href=\"/about-the-cfia/permits-licences-and-approvals/eng/1395348112901/1395348237219\">Permits, Licences and Approvals</a>,\n <a href=\"/about-the-cfia/accountability/eng/1299776530447/1299776743937\">Accountability</a>,\n <a href=\"/about-the-cfia/inspection-capacity/eng/1521596857393/1521596949071\">Inspection Capacity</a>,\n <a href=\"/about-the-cfia/organizational-information/eng/1323224617636/1323224814073\">Organizational Information </a>,\n <a href=\"/about-the-cfia/strategic-priorities/eng/1374871172385/1374871197211\">Strategic Priorities</a>,\n <a href=\"/about-the-cfia/science/eng/1494875229872/1494875261582\">Science</a>,\n <a href=\"/about-the-cfia/the-cfia-chronicle/eng/1528770452012/1528770503117\">The <abbr title=\"Canadian Food Inspection Agency\">CFIA</abbr> Chronicle</a>,\n <a href=\"/about-the-cfia/newsroom/eng/1299073792503/1299076004509\">Newsroom</a>]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Nice. But we don't want ALL the links, just the ones that go to the deails of the recalls. By inspecting the page with Chrome's developer tools, we see that those links are inside a table with classes \"table table-striped table-hover\" and then inside the `<tbody>` node.\n\n<img src=\"img/link_table.png\">\n\nSo let's get those."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "links_table = soup.find(\"table\", class_ = \"table table-striped table-hover\")\n\nprint(links_table.prettify()[:500])",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<table class=\"table table-striped table-hover\">\n <caption class=\"text-left mrgn-bttm-sm\">\n  Food Recall Warnings and Allergy Alerts\n </caption>\n <thead>\n  <tr>\n   <th>\n    Posted\n   </th>\n   <th>\n    Recall\n   </th>\n   <th>\n    Class\n   </th>\n   <th>\n    Distribution\n   </th>\n  </tr>\n </thead>\n <tbody>\n  <!-- WCMS:RECALL-LIST-ITEM BEGIN -->\n  <tr>\n   <td>\n    2015-12-30\n   </td>\n   <td>\n    <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-30/eng/1451509015225/14515\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# links_table is also a BeautifulSoup object with all the same methods.\n# We can inspect its classes, for instance\n\nlinks_table[\"class\"]",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "['table', 'table-striped', 'table-hover']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We can see how many children nodes it has\n\nlen(links_table.contents)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now that we isolated the table with the links we want, let's get just the links.\nlinks = links_table.find_all(\"a\")\nlinks[:10]",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "[<a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-30/eng/1451509015225/1451509020606\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-30\">Updated Food Recall Warning (Allergen) - <span lang=\"zh\">Bingquan</span> brand Ladies' Soy Drink recalled due to undeclared milk</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-23/eng/1450914001134/1450914006613\" title=\"Food Recall Warning (Allergen) - 2015-12-23\">Food Recall Warning (Allergen) - Chicken Veggie Pie and <span lang=\"fr\">Tourti√®re</span> recalled due to undeclared milk and soy</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-23b/eng/1450922998982/1450923004317\" title=\"Food Recall Warning (Allergen) - 2015-12-23\">Food Recall Warning (Allergen) - Ho Ho Ho Food Products brand seafood products recalled due to undeclared egg</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-21/eng/1450731556477/1450731562281\" title=\"Food Recall Warning (Allergen) - 2015-12-21\">Food Recall Warning (Allergen) - Certain Glosette brand Raisins recalled due to undeclared peanut</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18c/eng/1450507573695/1450507578970\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-19\">Updated Food Recall Warning (Allergen) - Hai Pa Wang brand Frozen Fish Dumpling with Cuttlefish recalled due to undeclared egg and wheat</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18b/eng/1450504262018/1450504267896\" title=\"Food Recall Warning (Allergen) - 2015-12-19\">Food Recall Warning (Allergen) - Soyspring brand Ladies' Soybean Drink recalled due to undeclared milk</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-19b/eng/1450590325536/1450590331480\" title=\"Food Recall Warning - 2015-12-19\">Food Recall Warning - Certain Bothwell brand shredded cheese products recalled due to <i lang=\"la\">Listeria monocytogenes</i></a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-19/eng/1450547016808/1450547021925\" title=\"Food Recall Warning - 2015-12-19\">Food Recall Warning - Ding Ho Foods brand seafood products recalled due to undeclared egg</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18/eng/1450486912533/1450486918327\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-18\">Updated Food Recall Warning (Allergen) - Taramosalata products recalled due to undeclared milk and sulphites</a>,\n <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-17/eng/1450413418110/1450413423680\" title=\"Food Recall Warning (Allergen) - 2015-12-17\">Food Recall Warning (Allergen) - Taramosalata products recalled due to undeclared milk and sulphites</a>]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Great, we can start accessing each recall page. We just want the `href` attribute of the link, so we'll use BeautifulSoup to isolate it and pass it to requests."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for link in links:\n    href = link.get(\"href\")\n    r_details = requests.get(\"http://www.inspection.gc.ca/\" + href)\n    soup_details = BeautifulSoup(r_details.content, \"lxml\")",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "OK, we got all the links to all recall details for a given year.\n\nNow we need to study the pages for each recall to see what data we want to extract from it.\n\nFor this exercise, all I want are the details on the top of the page: date, reason for recall, hazard class, the company name, and where the product was distributed.\n\nAgain, inspecting that element shows it's in a `<dl>` element with a class of \"dl-horizontal\". The details we want are in child `<dd>` nodes.\n\n<img src=\"img/details.png\">\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, let's load a single recall page to see how it works."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "r = requests.get(\"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2014-03-31/eng/1396319875632/1396319886479\")\nsoup = BeautifulSoup(r.content)\n\n# Right away we can isolate the dl and find all the dd's\ndl = soup.find(\"dl\", class_=\"dl-horizontal\")\ndetails = dl.find_all(\"dd\")\ndetails",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/plain": "[<dd class=\"mrgn-bttm-sm\">March 31, 2014</dd>,\n <dd class=\"mrgn-bttm-sm\">Allergen - Milk</dd>,\n <dd class=\"mrgn-bttm-sm\">Class 1</dd>,\n <dd class=\"mrgn-bttm-sm\">Altra Foods <abbr title=\"Incorporated\">Inc.</abbr>, Candy &amp; Chocolate Creations, Vancouver Judaica Group</dd>,\n <dd class=\"mrgn-bttm-sm\">Possibly National</dd>,\n <dd class=\"mrgn-bttm-sm\">Consumer</dd>,\n <dd class=\"mrgn-bttm-sm\">8747, 8755, 8760</dd>]"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Beautiful. Let's get the text content of each up to the line we care about (distribution). Look how easy it is:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for item in details[:5]:\n    print item.text\n\n# Again, item is a BeautifulSoup object with all the methods available",
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "March 31, 2014\nAllergen - Milk\nClass 1\nAltra Foods Inc., Candy & Chocolate Creations, Vancouver Judaica Group\nPossibly National\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can save each item to a variable and store it in the database of choice. I prefer CSVs. So putting everything together:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport time\nimport pandas as pd\n\n# Concatenate the URL up to 'ay=' with the year we want, then the tail end\nurl_head = \"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=\"\nurl_tail = \"&fr=0&fc=0&fd=0&ft=1\"\n\ndef cycle_years(start_year, end_year, doc):\n    for year in range(start_year, end_year + 1):\n        print(\"Getting data for year {}\".format(str(year)))\n        r = requests.get(url_head + str(year) + url_tail)\n        # Load the raw HTML into a BeautifulSoup object\n        soup = BeautifulSoup(r.content, \"lxml\")\n        links_table = soup.find(\"table\", class_ = \"table table-striped table-hover\")\n        links = links_table.find_all(\"a\")\n        for link in links:\n            href = link.get(\"href\")\n            scrape_details(href, doc)\n            time.sleep(1)\n\ndef scrape_details(href, doc):\n    r = requests.get(\"http://www.inspection.gc.ca/\" + href)\n    soup = BeautifulSoup(r.content, \"lxml\")\n    dl = soup.find(\"dl\", class_=\"dl-horizontal\")\n    details = dl.find_all(\"dd\")\n    # Convert text date into a Python datetime object\n    date = datetime.strptime(details[0].text, \"%B %d, %Y\")\n    doc['date'] += [date]\n    doc['reason'].append(details[1].text)\n    doc['recall_class'].append(details[2].text)\n    doc['company'].append(details[3].text)\n    doc['distribution'].append(details[4].text)\n    print(\"   Recall date: {}\".format(date.strftime(\"%d/%m/%Y\")))\n    \n\n# initializing empty doc to collect data\ndoc = {\n    'date':[],\n    'reason':[],\n    'recall_class':[],\n    'company':[],\n    'distribution':[]\n}\n\ncycle_years(2018, 2019,doc)\n\n",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Getting data for year 2018\n   Recall date: 31/12/2018\n   Recall date: 24/12/2018\n   Recall date: 20/12/2018\n   Recall date: 15/12/2018\n   Recall date: 14/12/2018\n   Recall date: 10/12/2018\n   Recall date: 05/12/2018\n   Recall date: 04/12/2018\n   Recall date: 29/11/2018\n   Recall date: 28/11/2018\n   Recall date: 21/11/2018\n   Recall date: 21/11/2018\n   Recall date: 16/11/2018\n   Recall date: 16/11/2018\n   Recall date: 14/11/2018\n   Recall date: 09/11/2018\n   Recall date: 08/11/2018\n   Recall date: 02/11/2018\n   Recall date: 31/10/2018\n   Recall date: 26/10/2018\n   Recall date: 24/10/2018\n   Recall date: 22/10/2018\n   Recall date: 22/10/2018\n   Recall date: 19/10/2018\n   Recall date: 18/10/2018\n   Recall date: 17/10/2018\n   Recall date: 15/10/2018\n   Recall date: 12/10/2018\n   Recall date: 11/10/2018\n   Recall date: 09/10/2018\n   Recall date: 04/10/2018\n   Recall date: 03/10/2018\n   Recall date: 02/10/2018\n   Recall date: 28/09/2018\n   Recall date: 28/09/2018\n   Recall date: 27/09/2018\n   Recall date: 25/09/2018\n   Recall date: 24/09/2018\n   Recall date: 23/09/2018\n   Recall date: 23/09/2018\n   Recall date: 22/09/2018\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7be2d293959a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mcycle_years\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-7be2d293959a>\u001b[0m in \u001b[0;36mcycle_years\u001b[0;34m(start_year, end_year, doc)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mscrape_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "                                             company       date  \\\n0                              Cape Breton Fudge Co. 2018-12-31   \n1  Buy Low Foods Ltd., Loblaw Companies Limited, ... 2018-12-24   \n2                            Bulk Barn Foods Limited 2018-12-20   \n3  Courchesne Larose Lt√©e, Fruits et L√©gumes Ga√©t... 2018-12-15   \n4  Buy Low Foods Ltd., Loblaw Companies Limited, ... 2018-12-14   \n\n                                        distribution  \\\n0                                        Nova Scotia   \n1                                           National   \n2                                           National   \n3  New Brunswick, Newfoundland and Labrador, Nova...   \n4                                           National   \n\n                              reason recall_class  \n0                Allergen - Tree Nut      Class 1  \n1         Microbiological - Listeria      Class 1  \n2                    Allergen - Milk      Class 1  \n3  Microbiological - E. coli O157:H7      Class 1  \n4         Microbiological - Listeria      Class 1  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company</th>\n      <th>date</th>\n      <th>distribution</th>\n      <th>reason</th>\n      <th>recall_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cape Breton Fudge Co.</td>\n      <td>2018-12-31</td>\n      <td>Nova Scotia</td>\n      <td>Allergen - Tree Nut</td>\n      <td>Class 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Buy Low Foods Ltd., Loblaw Companies Limited, ...</td>\n      <td>2018-12-24</td>\n      <td>National</td>\n      <td>Microbiological - Listeria</td>\n      <td>Class 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bulk Barn Foods Limited</td>\n      <td>2018-12-20</td>\n      <td>National</td>\n      <td>Allergen - Milk</td>\n      <td>Class 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Courchesne Larose Lt√©e, Fruits et L√©gumes Ga√©t...</td>\n      <td>2018-12-15</td>\n      <td>New Brunswick, Newfoundland and Labrador, Nova...</td>\n      <td>Microbiological - E. coli O157:H7</td>\n      <td>Class 1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Buy Low Foods Ltd., Loblaw Companies Limited, ...</td>\n      <td>2018-12-14</td>\n      <td>National</td>\n      <td>Microbiological - Listeria</td>\n      <td>Class 1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "let's save the scraped data to a pandas data frame"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.DataFrame(doc)\n\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "we successfuly scraped data from the web! now we can get to the fun part!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}