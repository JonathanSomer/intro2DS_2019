{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping websites with BeautifulSoup\n",
    "\n",
    "This is a basic walkthrough of harvesting data from websites that have infromation spread across multiple pages and inside links.\n",
    "\n",
    "We'll be scraping the database of food recall warnings from the [Canadian Food Inspection Agency](http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221).\n",
    "\n",
    "First, the set-up. This tutorial uses Python 2.7 and three modules that need to be installed:\n",
    "\n",
    "* [requests](http://docs.python-requests.org/en/latest/) for HTTP\n",
    "* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for parsing HTML\n",
    "\n",
    "\n",
    "## Mind your ethics\n",
    "\n",
    "Scraping websites carry a few ethical questions: does the site's terms of use forbid scraping? Is it copyright infringement to republish the data? Are you punishing the servers with too many requests?\n",
    "\n",
    "web scraping is in somewhat of a gray moral area. on one side you are harvesting data that is not private and is available to you through the webpage, but you are creating trafic on the websites server, without compensating the owner by complying with the sites buisiness model (allowing them to present ads?)\n",
    "so please keep these points in mind and don't be surprised if the site owner will try to block your scraper :)\n",
    "\n",
    "## Study the site structure\n",
    "\n",
    "First, we need to spend time on the site to understand how the website is built. What's the URL structure? What's the general hirarchical structure of the webpage?\n",
    "\n",
    "By poking around a bit and selecting a year in the top menu, we see that the site lists all warnings for that year. Good, less pagination work for us.\n",
    "\n",
    "That year is also specified in the '`ay=`' parameter in the URL. And if you change that parameter to another year, all the warnins for that year are loaded.\n",
    "\n",
    "<img src=\"img/url.png\">\n",
    "\n",
    "This will help us cycle through the different years.\n",
    "\n",
    "So let's set up our first task, which is cycling through the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Concatenate the URL up to 'ay=' with the year we want, then the tail end\n",
    "url_head = \"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=\"\n",
    "url_tail = \"&fr=0&fc=0&fd=0&ft=1\"\n",
    "\n",
    "def cycle_years(start_year, end_year):\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        r = requests.get(url_head + str(year) + url_tail)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leave this for now and come back to it later. \n",
    "\n",
    "We'll first look at how BeautifulSoup works at parsing a DOM. Let's load just the index page for 2015 recalls and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=2015&fr=0&fc=0&fd=0&ft=1\")\n",
    "soup = BeautifulSoup(r.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 9]><html class=\"no-js lt-ie9\" lang=\"en\" dir=\"ltr\"><![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Complete listing of all recalls and allergy alerts  - Canadian Food Inspection Agency\n",
      "  </title>\n",
      "  <meta content=\"Complete listing of all recalls and allergy alerts\" name=\"dcterms.title\"/>\n",
      "  <meta content=\"Canadian Food Inspection Agency\" name=\"description\"/>\n",
      "  <meta content=\"Canadian Food Inspection Agency\" name=\"dcterms.description\"/>\n",
      "  <meta content=\"Canadian Food Inspection Agency\" name=\"keywords\"/>\n",
      "  <meta content=\"inspection\" name=\"dcterms.subject\" title=\"gccore\"/>\n",
      "  <meta content=\"Government of Canada,Canadian Food Inspection Agency\" name=\"dcterms.creator\"/>\n",
      "  <meta content=\"eng\" name=\"dcterms.language\" title=\"ISO639-2\"/>\n",
      "  <meta content=\"2012-10-29 10:06:27\" name=\"dcterms.issued\" title=\"W3\n"
     ]
    }
   ],
   "source": [
    "# The variable soup is a BeautifulSoup object containing the entire HTML document.\n",
    "\n",
    "# Use prettify() to pretty-print the document\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Complete listing of all recalls and allergy alerts  - About the Canadian Food Inspection Agency - Canadian Food Inspection Agency</title>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access some elements directly (for example page title)\n",
    "\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<link href=\"/DAM/PresentationService/WET/4.0.19/theme-gcwu-fegc/assets/favicon.ico\" rel=\"icon\" type=\"image/x-icon\"/>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can keep going deeper, and really target the node we want. \n",
    "\n",
    "soup.head.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"wb-sl\" href=\"#wb-cont\">Skip to main content</a>,\n",
       " <a class=\"wb-sl\" href=\"#wb-info\">Skip to \"About this site\"</a>,\n",
       " <a class=\"wb-sl\" href=\"#wb-sec\">Skip to section menu</a>,\n",
       " <a href=\"https://www.canada.ca/en.html\" rel=\"external\">Canada.ca</a>,\n",
       " <a href=\"https://www.canada.ca/en/services.html\" rel=\"external\">Services</a>,\n",
       " <a href=\"https://www.canada.ca/en/government/dept.html\" rel=\"external\">Departments</a>,\n",
       " <a href=\"/au-sujet-de-l-acia/salle-de-nouvelles/avis-de-rappel-d-aliments/liste-complete/fra/1351519587174/1351519588221?ay=2015&amp;fr=0&amp;fc=0&amp;fd=0&amp;ft=1\" lang=\"fr\">Fran√ßais</a>,\n",
       " <a aria-controls=\"mb-pnl\" class=\"overlay-lnk btn btn-sm btn-default\" href=\"#mb-pnl\" role=\"button\" title=\"Search and menus\"><span class=\"glyphicon glyphicon-search\"><span class=\"glyphicon glyphicon-th-list\"><span class=\"wb-inv\">Search and menus</span></span></span></a>,\n",
       " <a href=\"/eng/1297964599443/1297965645317\">Canadian Food Inspection Agency</a>,\n",
       " <a class=\"item\" href=\"#aboutthecfia\">About the <abbr title=\"Canadian Food Inspection Agency\">CFIA</abbr></a>,\n",
       " <a href=\"/about-the-cfia/acts-and-regulations/eng/1299846777345/1299847442232\">Acts and Regulations</a>,\n",
       " <a href=\"/about-the-cfia/contact-the-cfia/eng/1546627816321/1546627838025\">Contact Us</a>,\n",
       " <a href=\"/about-the-cfia/permits-licences-and-approvals/eng/1395348112901/1395348237219\">Permits, Licences and Approvals</a>,\n",
       " <a href=\"/about-the-cfia/accountability/eng/1299776530447/1299776743937\">Accountability</a>,\n",
       " <a href=\"/about-the-cfia/inspection-capacity/eng/1521596857393/1521596949071\">Inspection Capacity</a>,\n",
       " <a href=\"/about-the-cfia/organizational-information/eng/1323224617636/1323224814073\">Organizational Information </a>,\n",
       " <a href=\"/about-the-cfia/strategic-priorities/eng/1374871172385/1374871197211\">Strategic Priorities</a>,\n",
       " <a href=\"/about-the-cfia/science/eng/1494875229872/1494875261582\">Science</a>,\n",
       " <a href=\"/about-the-cfia/the-cfia-chronicle/eng/1528770452012/1528770503117\">The <abbr title=\"Canadian Food Inspection Agency\">CFIA</abbr> Chronicle</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/eng/1299073792503/1299076004509\">Newsroom</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say we want to get all the hyperlinks into a list:\n",
    "\n",
    "soup.find_all(\"a\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. But we don't want ALL the links, just the ones that go to the deails of the recalls. By inspecting the page with Chrome's developer tools, we see that those links are inside a table with classes \"table table-striped table-hover\" and then inside the `<tbody>` node.\n",
    "\n",
    "<img src=\"img/link_table.png\">\n",
    "\n",
    "So let's get those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"table table-striped table-hover\">\n",
      " <caption class=\"text-left mrgn-bttm-sm\">\n",
      "  Food Recall Warnings and Allergy Alerts\n",
      " </caption>\n",
      " <thead>\n",
      "  <tr>\n",
      "   <th>\n",
      "    Posted\n",
      "   </th>\n",
      "   <th>\n",
      "    Recall\n",
      "   </th>\n",
      "   <th>\n",
      "    Class\n",
      "   </th>\n",
      "   <th>\n",
      "    Distribution\n",
      "   </th>\n",
      "  </tr>\n",
      " </thead>\n",
      " <tbody>\n",
      "  <!-- WCMS:RECALL-LIST-ITEM BEGIN -->\n",
      "  <tr>\n",
      "   <td>\n",
      "    2015-12-30\n",
      "   </td>\n",
      "   <td>\n",
      "    <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-30/eng/1451509015225/14515\n"
     ]
    }
   ],
   "source": [
    "links_table = soup.find(\"table\", class_ = \"table table-striped table-hover\")\n",
    "\n",
    "print(links_table.prettify()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table', 'table-striped', 'table-hover']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# links_table is also a BeautifulSoup object with all the same methods.\n",
    "# We can inspect its classes, for instance\n",
    "\n",
    "links_table[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see how many children nodes it has\n",
    "\n",
    "len(links_table.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-30/eng/1451509015225/1451509020606\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-30\">Updated Food Recall Warning (Allergen) - <span lang=\"zh\">Bingquan</span> brand Ladies' Soy Drink recalled due to undeclared milk</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-23/eng/1450914001134/1450914006613\" title=\"Food Recall Warning (Allergen) - 2015-12-23\">Food Recall Warning (Allergen) - Chicken Veggie Pie and <span lang=\"fr\">Tourti√®re</span> recalled due to undeclared milk and soy</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-23b/eng/1450922998982/1450923004317\" title=\"Food Recall Warning (Allergen) - 2015-12-23\">Food Recall Warning (Allergen) - Ho Ho Ho Food Products brand seafood products recalled due to undeclared egg</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-21/eng/1450731556477/1450731562281\" title=\"Food Recall Warning (Allergen) - 2015-12-21\">Food Recall Warning (Allergen) - Certain Glosette brand Raisins recalled due to undeclared peanut</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18c/eng/1450507573695/1450507578970\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-19\">Updated Food Recall Warning (Allergen) - Hai Pa Wang brand Frozen Fish Dumpling with Cuttlefish recalled due to undeclared egg and wheat</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18b/eng/1450504262018/1450504267896\" title=\"Food Recall Warning (Allergen) - 2015-12-19\">Food Recall Warning (Allergen) - Soyspring brand Ladies' Soybean Drink recalled due to undeclared milk</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-19b/eng/1450590325536/1450590331480\" title=\"Food Recall Warning - 2015-12-19\">Food Recall Warning - Certain Bothwell brand shredded cheese products recalled due to <i lang=\"la\">Listeria monocytogenes</i></a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-19/eng/1450547016808/1450547021925\" title=\"Food Recall Warning - 2015-12-19\">Food Recall Warning - Ding Ho Foods brand seafood products recalled due to undeclared egg</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-18/eng/1450486912533/1450486918327\" title=\"Updated Food Recall Warning (Allergen) - 2015-12-18\">Updated Food Recall Warning (Allergen) - Taramosalata products recalled due to undeclared milk and sulphites</a>,\n",
       " <a href=\"/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2015-12-17/eng/1450413418110/1450413423680\" title=\"Food Recall Warning (Allergen) - 2015-12-17\">Food Recall Warning (Allergen) - Taramosalata products recalled due to undeclared milk and sulphites</a>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we isolated the table with the links we want, let's get just the links.\n",
    "links = links_table.find_all(\"a\")\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can start accessing each recall page. We just want the `href` attribute of the link, so we'll use BeautifulSoup to isolate it and pass it to requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    href = link.get(\"href\")\n",
    "    r_details = requests.get(\"http://www.inspection.gc.ca/\" + href)\n",
    "    soup_details = BeautifulSoup(r_details.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we got all the links to all recall details for a given year.\n",
    "\n",
    "Now we need to study the pages for each recall to see what data we want to extract from it.\n",
    "\n",
    "For this exercise, all I want are the details on the top of the page: date, reason for recall, hazard class, the company name, and where the product was distributed.\n",
    "\n",
    "Again, inspecting that element shows it's in a `<dl>` element with a class of \"dl-horizontal\". The details we want are in child `<dd>` nodes.\n",
    "\n",
    "<img src=\"img/details.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's load a single recall page to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<dd class=\"mrgn-bttm-sm\">March 31, 2014</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">Allergen - Milk</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">Class 1</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">Altra Foods <abbr title=\"Incorporated\">Inc.</abbr>, Candy &amp; Chocolate Creations, Vancouver Judaica Group</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">Possibly National</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">Consumer</dd>,\n",
       " <dd class=\"mrgn-bttm-sm\">8747, 8755, 8760</dd>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/2014-03-31/eng/1396319875632/1396319886479\")\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "# Right away we can isolate the dl and find all the dd's\n",
    "dl = soup.find(\"dl\", class_=\"dl-horizontal\")\n",
    "details = dl.find_all(\"dd\")\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful. Let's get the text content of each up to the line we care about (distribution). Look how easy it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 31, 2014\n",
      "Allergen - Milk\n",
      "Class 1\n",
      "Altra Foods Inc., Candy & Chocolate Creations, Vancouver Judaica Group\n",
      "Possibly National\n"
     ]
    }
   ],
   "source": [
    "for item in details[:5]:\n",
    "    print(item.text)\n",
    "\n",
    "# Again, item is a BeautifulSoup object with all the methods available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save each item to a variable and store it in the database of choice. I prefer CSVs. So putting everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for year 2018\n",
      "   Recall date: 31/12/2018\n",
      "   Recall date: 24/12/2018\n",
      "   Recall date: 20/12/2018\n",
      "   Recall date: 15/12/2018\n",
      "   Recall date: 14/12/2018\n",
      "   Recall date: 10/12/2018\n",
      "   Recall date: 05/12/2018\n",
      "   Recall date: 04/12/2018\n",
      "   Recall date: 29/11/2018\n",
      "   Recall date: 28/11/2018\n",
      "   Recall date: 21/11/2018\n",
      "   Recall date: 21/11/2018\n",
      "   Recall date: 16/11/2018\n",
      "   Recall date: 16/11/2018\n",
      "   Recall date: 14/11/2018\n",
      "   Recall date: 09/11/2018\n",
      "   Recall date: 08/11/2018\n",
      "   Recall date: 02/11/2018\n",
      "   Recall date: 31/10/2018\n",
      "   Recall date: 26/10/2018\n",
      "   Recall date: 24/10/2018\n",
      "   Recall date: 22/10/2018\n",
      "   Recall date: 22/10/2018\n",
      "   Recall date: 19/10/2018\n",
      "   Recall date: 18/10/2018\n",
      "   Recall date: 17/10/2018\n",
      "   Recall date: 15/10/2018\n",
      "   Recall date: 12/10/2018\n",
      "   Recall date: 11/10/2018\n",
      "   Recall date: 09/10/2018\n",
      "   Recall date: 04/10/2018\n",
      "   Recall date: 03/10/2018\n",
      "   Recall date: 02/10/2018\n",
      "   Recall date: 28/09/2018\n",
      "   Recall date: 28/09/2018\n",
      "   Recall date: 27/09/2018\n",
      "   Recall date: 25/09/2018\n",
      "   Recall date: 24/09/2018\n",
      "   Recall date: 23/09/2018\n",
      "   Recall date: 23/09/2018\n",
      "   Recall date: 22/09/2018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7be2d293959a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m }\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mcycle_years\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7be2d293959a>\u001b[0m in \u001b[0;36mcycle_years\u001b[0;34m(start_year, end_year, doc)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mscrape_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the URL up to 'ay=' with the year we want, then the tail end\n",
    "url_head = \"http://www.inspection.gc.ca/about-the-cfia/newsroom/food-recall-warnings/complete-listing/eng/1351519587174/1351519588221?ay=\"\n",
    "url_tail = \"&fr=0&fc=0&fd=0&ft=1\"\n",
    "\n",
    "def cycle_years(start_year, end_year, doc):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(\"Getting data for year {}\".format(str(year)))\n",
    "        r = requests.get(url_head + str(year) + url_tail)\n",
    "        # Load the raw HTML into a BeautifulSoup object\n",
    "        soup = BeautifulSoup(r.content, \"lxml\")\n",
    "        links_table = soup.find(\"table\", class_ = \"table table-striped table-hover\")\n",
    "        links = links_table.find_all(\"a\")\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            scrape_details(href, doc)\n",
    "            time.sleep(1)\n",
    "\n",
    "def scrape_details(href, doc):\n",
    "    r = requests.get(\"http://www.inspection.gc.ca/\" + href)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    dl = soup.find(\"dl\", class_=\"dl-horizontal\")\n",
    "    details = dl.find_all(\"dd\")\n",
    "    # Convert text date into a Python datetime object\n",
    "    date = datetime.strptime(details[0].text, \"%B %d, %Y\")\n",
    "    doc['date'] += [date]\n",
    "    doc['reason'].append(details[1].text)\n",
    "    doc['recall_class'].append(details[2].text)\n",
    "    doc['company'].append(details[3].text)\n",
    "    doc['distribution'].append(details[4].text)\n",
    "    print(\"   Recall date: {}\".format(date.strftime(\"%d/%m/%Y\")))\n",
    "    \n",
    "\n",
    "# initializing empty doc to collect data\n",
    "doc = {\n",
    "    'date':[],\n",
    "    'reason':[],\n",
    "    'recall_class':[],\n",
    "    'company':[],\n",
    "    'distribution':[]\n",
    "}\n",
    "\n",
    "cycle_years(2018, 2019,doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>distribution</th>\n",
       "      <th>reason</th>\n",
       "      <th>recall_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cape Breton Fudge Co.</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Nova Scotia</td>\n",
       "      <td>Allergen - Tree Nut</td>\n",
       "      <td>Class 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Low Foods Ltd., Loblaw Companies Limited, ...</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>National</td>\n",
       "      <td>Microbiological - Listeria</td>\n",
       "      <td>Class 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bulk Barn Foods Limited</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>National</td>\n",
       "      <td>Allergen - Milk</td>\n",
       "      <td>Class 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Courchesne Larose Lt√©e, Fruits et L√©gumes Ga√©t...</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>New Brunswick, Newfoundland and Labrador, Nova...</td>\n",
       "      <td>Microbiological - E. coli O157:H7</td>\n",
       "      <td>Class 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buy Low Foods Ltd., Loblaw Companies Limited, ...</td>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>National</td>\n",
       "      <td>Microbiological - Listeria</td>\n",
       "      <td>Class 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             company       date  \\\n",
       "0                              Cape Breton Fudge Co. 2018-12-31   \n",
       "1  Buy Low Foods Ltd., Loblaw Companies Limited, ... 2018-12-24   \n",
       "2                            Bulk Barn Foods Limited 2018-12-20   \n",
       "3  Courchesne Larose Lt√©e, Fruits et L√©gumes Ga√©t... 2018-12-15   \n",
       "4  Buy Low Foods Ltd., Loblaw Companies Limited, ... 2018-12-14   \n",
       "\n",
       "                                        distribution  \\\n",
       "0                                        Nova Scotia   \n",
       "1                                           National   \n",
       "2                                           National   \n",
       "3  New Brunswick, Newfoundland and Labrador, Nova...   \n",
       "4                                           National   \n",
       "\n",
       "                              reason recall_class  \n",
       "0                Allergen - Tree Nut      Class 1  \n",
       "1         Microbiological - Listeria      Class 1  \n",
       "2                    Allergen - Milk      Class 1  \n",
       "3  Microbiological - E. coli O157:H7      Class 1  \n",
       "4         Microbiological - Listeria      Class 1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "let's save the scraped data to a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we successfuly scraped data from the web! now we can get to the fun part!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
