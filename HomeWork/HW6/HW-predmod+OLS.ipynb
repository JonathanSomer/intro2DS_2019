{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6: predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions\n",
    "\n",
    "our course will be using an automatic grading system. <br>\n",
    "after each question there will appear a code block with some prepared code to add your answer to a dictionary that will be sent to the course server for grading. <br>\n",
    "please do not edit any code other than in placeholders marked `#### your code here ####` <br>\n",
    "__don't forget to run the code block after you write your answer.__\n",
    "\n",
    "\n",
    "you can add code blocks wherever you want in order to interact with datasets and play with your own code. <br>\n",
    "in the next code block plase fill in your id number and email account in the appropriate placees. <br>\n",
    "and __don't forget to run the block!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['HW'] = 'HW5'\n",
    "ans['id_number'] = #### your id here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1)\n",
    "Which of the following \"real life\" problems does not involve predictive modeling to perform: \n",
    "\n",
    "1. Identify spam\n",
    "\n",
    "2. Find out which lifestyle variables (weight, exercise,...) can predict blood pressure\n",
    "\n",
    "3. In a user database, find \"clusters\" of similar users\n",
    "\n",
    "4. In a user database, find users who are likely to purchase product X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q1'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "Recall we defined the loss function for prediction $L(y,\\hat{y})$ as the \"cost\" of predicting $\\hat{y}$ when the truth is $y$. Assume we are in a regression problem of predicting how much of product X a store should order. Therefore, errors of underestimation (resulting in unmet demand) are much worse than overestimation (resulting in surplus in the store). What property should $L$ have to reflect this asymmetry?\n",
    "\n",
    "1. For any $a \\in \\mathbb R$, $L(a, a+c) > L(a+c,a),\\;\\forall c \\in \\mathbb R.$\n",
    "\n",
    "2. For any $a \\in \\mathbb R$, $L(a, a+c) = L(a+c,a),\\;\\forall c\\in \\mathbb R.$\n",
    "\n",
    "3. For any $a \\in \\mathbb R$, $L(a, a+c) > L(a+c,a),\\;\\forall c>0.$\n",
    "\n",
    "4. For any $a \\in \\mathbb R$, $L(a, a+c) < L(a+c,a),\\;\\forall c>0.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "Consider the OLS regression models like we built in class on Netflix data, with 80%-20% training-test division. First build a model which uses only the movie The Rock (column 12 in X) + constant vs the model which uses only the movie Forrest Gump (column 5) + constant. Build both models on the 80% training and compare their MSE on the 20% test. Repeat this at least 10 times with random training-test divisions, enough times to answer the following: Which movie gives better prediction of Miss Congeniality score on the test? \n",
    "\n",
    "1. They both win about half the times\n",
    "\n",
    "2. The Rock consistently does better\n",
    "\n",
    "3. Forrest Gump consistently does better\n",
    "\n",
    "4. Not enough information to decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "Now add the other 12 fully observed movies to each model, giving you two models, each with 13 movies + constant as explanatory variables (One model has all 14 movies **except** Forrest Gump, one all 14 except The Rock). Call the first one the \"NoGump\" model and the second the \"NoRock\" model. Repeat the previous exercise of doing many random training-test divisions, fitting each model and evaluating them on the test. Which model does better now: \n",
    "\n",
    "1. They both win about half the times\n",
    "\n",
    "2. NoGump (which includes The Rock) consistently does better\n",
    "\n",
    "3. NoRock consistently does better\n",
    "\n",
    "4. They both crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q4'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5)\n",
    "Regardless of what you answered in the two previous problems, is it possible in theory that The Rock does better than Forrest Gump in the first setup while NoRock does better than NoGump in the second? Why?\n",
    "\n",
    "1. It is impossible, since if The Rock is a better predictor, it will be better with any combination of other variables\n",
    "\n",
    "2. It is possible, since The Rock may be a better predictor on its own, but less important in the presence of the other movies\n",
    "\n",
    "3. It is impossible, since The Rock is not in the model NoRock and Forrest Gump is not in the model NoGump\n",
    "\n",
    "4. it is possible, since the training and test set are different from each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q5'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6)\n",
    "Given a training set with $n$ training observations, $m$ test observations, response $y$, and $p=3$ explanatory variables, denoted $a,b,c$. I build  several OLS regression models: model mod1 of $y$ as a function of $a$ only, model mod2 of $y$ as a function of $a,b$ both, and model mod3 of $y$ as a function of $a,c$ both. Denote the training error (RSS of OLS solution) of the models by RSS1, RSS2, RSS3 respectively, which of the following holds: \n",
    "\n",
    "1. $RSS1 \\ge RSS2$ and $RSS1 \\ge RSS3$\n",
    "\n",
    "2. $RSS1 \\le RSS2$ and $RSS1 \\le RSS3$\n",
    "\n",
    "3. $RSS1 \\ge RSS2$ and $RSS2 = RSS3$\n",
    "\n",
    "4. We cannot say anything according to the given information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q6'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7)\n",
    "In the same setting, denote the test sqaured error (i.e., using $L(y,\\hat{y}) = (y-\\hat{y})^2$) of the three models by MSE1, MSE2, MSE3 respectively. What can we say about the relations between these test MSEs? \n",
    "\n",
    "1. $MSE1 \\ge MSE2$ and $MSE1 \\ge MSE3$\n",
    "\n",
    "2. $MSE1 \\le MSE2$ and $MSE1 \\le MSE3$\n",
    "\n",
    "3. $MSE1 \\ge MSE2$ and $MSE2 = MSE3$\n",
    "\n",
    "4. We cannot say anything according to the given information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q7'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### programming task\n",
    "use the next code block to import the \"brain size\" dataset.<br>\n",
    "the dataset details the score of a verbal IQ test and brain size measurments of 20 female collage students.<br>\n",
    "(brain size is measured as number of thousands of pixels in a MRI scan)<br>\n",
    "((yes, this is a real dataset, some people have too much spare time and free access to MRI machines..))\n",
    "<br><br>\n",
    "fit a linear regression model to predict 'VerbalIQ' as a function of 'Brain Size'. (include an intercept) <br>\n",
    "do __not__ split the data to train-test or CV.<br>\n",
    "use scikit learn implementation, as demonstrated in the recitation\n",
    "<br><br>\n",
    "report the following stats:\n",
    "\n",
    "__Q8__ - intercept<br>\n",
    "__Q9__ - coeficiant of 'Brain Size'<br>\n",
    "__Q10__ - $R^2$ of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VerbalIQ</th>\n",
       "      <th>Brain Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>816.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132</td>\n",
       "      <td>951.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>928.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136</td>\n",
       "      <td>991.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>854.258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VerbalIQ  Brain Size\n",
       "0       132     816.932\n",
       "1       132     951.545\n",
       "2        90     928.799\n",
       "3       136     991.305\n",
       "4        90     854.258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('brain-size.txt', delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q8'] = #### your answer here ####\n",
    "ans['Q9'] = #### your answer here ####\n",
    "ans['Q10'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higgs boson \n",
    "The slides from the Higgs boson talk are available at https://cernbox.cern.ch/index.php/s/9taSheJCtUNLPgz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11)\n",
    "What is the goal of the Trigger mechanism (slides 54-55)\n",
    "\n",
    "1. To reduce the amount of stored data to a huge but manageable size \n",
    "2. To make measurements\n",
    "\n",
    "3. To test significance of the findings\n",
    "\n",
    "4. To decide when it is safe to operate the LHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q11'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12)\n",
    "Slide 21 discusses some important probabilistic approximations. The first is $Bin(n,p)\\approx Poisson (np)$ for big $n$ and small $p.$ Investigate about this relationship with pen and paper and/or through simulations. Assume $X\\sim Bin(n,p)$ and $Y\\sim Pois(np),$ which of the following holds: \n",
    "\n",
    "1. $\\mathbb E(X) > \\mathbb E(Y),\\; Var(X) > Var(Y).$\n",
    "\n",
    "2. $\\mathbb E(X) = \\mathbb E(Y),\\; Var(X) > Var(Y).$\n",
    "\n",
    "3. $\\mathbb E(X) = \\mathbb E(Y),\\; Var(X) < Var(Y).$\n",
    "\n",
    "4. $\\mathbb E(X) < \\mathbb E(Y),\\; Var(X) < Var(Y).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q12'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13)\n",
    "The second relationship discussed is that for large $\\lambda,\\; Pois(\\lambda) \\approx N(\\lambda, \\lambda).$ The slide quotes Stirling formula as a reason (you can check that out). Which of the following is a valid alternative argument for this approximation, based on what we have learned?\n",
    "\n",
    "1. The moments match properly, so the approximation is good by the Central Limit Theorem (CLT)\n",
    "\n",
    "2. The Poisson distribution describes a memoryless process with Exponential waiting time, as does the normal\n",
    "\n",
    "3. The Binomial approximation of Poisson from the previous problem + the CLT for Bernoulli \n",
    "\n",
    "4. There is no alternative valid argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q13'] = #### your answer here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish!\n",
    "\n",
    "to submit your HW please run this last code block and follow the instructions. <BR>\n",
    "this code will create a CSV file in the current directory on the azure notebooks project <br>\n",
    "please download it and submit it through moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 15:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
