{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSD 1: Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['id_number'] = 0\n",
    "ans['HW'] = 'CSD1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [ebay.com](ebay.com) and search for little boys t-shirts. The ebay website, like any modern website, is filled with text, images and links. But if you are using Google Chrome and you right-click on any page and choose \"View page source\" you will see the raw HTML script behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python library [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)'s job is to help you parse this raw HTML, to get what you want. Run the following piece of code, by pressing it and pressing the \"play\" icon in the above menu, or just Ctrl + Enter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"https://il.ebay.com/b/Boys-Short-Sleeve-Sleeve-Tops-T-Shirts-Sizes-4-Up/175521/bn_4278610?rt=nc&LH_ItemCondition=1000&LH_BIN=1&LH_PrefLoc=3&_pgn=1\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code imports Beautiful Soup, imports the requests library for handling web connections, assigns an ebay search results page address to a variable called `url`, \"requests\" this URL, stores the response in a variable called `r`, makes a `BeautifulSoup` object out of the response's `content`, and assigns it to a variable called `soup`.\n",
    "\n",
    "it is advised to visit the url using your browser, so you will have a visual understanding of what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the raw HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1) Replace the `### YOUR CODE HERE ###` comment to print __just the title__ of the page (as a string, without html tags).\n",
    "\n",
    "Hint 1: The [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "Hint 2: type `soup`, then `.`, then press TAB to get possible object members or methods from Jupyter\n",
    "\n",
    "Hint 3: Keep Calm and [Stack Overflow](https://stackoverflow.com/a/51550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_title = ### YOUR CODE HERE ###\n",
    "print(url_title)\n",
    "ans['Q1'] = url_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### links\n",
    "This is an HTML paragraph tag or element: `<p>This is a paragraph</p>`\n",
    "\n",
    "This is a hyperlink tag: `<a href=\"https://www.google.com/\">Google it!</a>`\n",
    "\n",
    "`find_all` links in a page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = ### YOUR CODE HERE ###\n",
    "print('all_links is a: ' + str(type(all_links)))\n",
    "print()\n",
    "print('first 5 elements in all_links:')\n",
    "print(all_links[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2) how many links are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = ### YOUR answer HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperlink tag has an attribute called `href` holding the link's address. In a BeautifulSoup element, to access this attribute you can think of an element as a dictionary and the attribute its key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(all_links[100]))\n",
    "print()\n",
    "print(all_links[100])\n",
    "print()\n",
    "print(all_links[100]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual dictionary of an element use the `attrs` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(all_links[100].attrs))\n",
    "print()\n",
    "print(all_links[100].attrs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images\n",
    "Find all image elements in our ebay page and put them in a variable called `images`. You might want to find out what is the [HTML tag for an image](https://www.w3schools.com/tags/tag_img.asp) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a `list` of all image titles from the `images` object, **except for the first one**. Print that list.\n",
    "\n",
    "Hint: `alt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_titles = [img['alt'] for img in images[1:]]\n",
    "print(image_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the attribute for an image JPEG file address?\n",
    "\n",
    "Some images have the attribute `src` and some `data-src`. This is one way to combine the two. Make sure you understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files_src = [img['src'] for img in images[1:]]\n",
    "image_files_datasrc = [img.get('data-src', None) for img in images[1:]]\n",
    "image_files = [src if datasrc is None else datasrc for src, datasrc in zip(image_files_src, image_files_datasrc)]\n",
    "image_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prices\n",
    "Let's find a shirt's price. \n",
    "go to the url in your browser and use the code inspection tool (F12) to look interactively at the url source code. \n",
    "find the element that holds price data.\n",
    "notice that the price may be nested within a few levels of htm tags. you are searching for the \"lowest\" level. that which holds the price directly.\n",
    "\n",
    "in our case it is a `span` element with a specific class\n",
    "\n",
    "#### Q3) what is the specific class for span elements holding the prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = ### YOUR answer HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_elements = soup.find_all('span', class_ = ### YOUR answer HERE ###)\n",
    "print(price_elements[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each of these `price_elements` we extract the actual price text with the `get_text` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_elements[1].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that all prices come with the \"ILS\" prefix and then the number. <br>\n",
    "also you can see some prices come as a range. <br>\n",
    "for this project we decided to simply take the minimum price of the range. <br>\n",
    "to do so we could split this string to its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_elements[1].get_text().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the second element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_elements[1].get_text().split(' ')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert it to a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(float(price_elements[1].get_text().split(' ')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the `parse_price` function so that in the end the `prices` variable will hold a list of all shirts prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_price(price_element):\n",
    "    try:\n",
    "        price = ### YOUR CODE HERE ###\n",
    "    except:\n",
    "        price = None\n",
    "    return price\n",
    "\n",
    "prices = [parse_price(price_e) for price_e in price_elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to actually download the shirts images! The following function accepts an image file address, a shirt title and the file name for the image and attempts to download the image to the current directory with the specified file name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, title, file_name):\n",
    "    try:\n",
    "        response = requests.get(url)    \n",
    "    except:\n",
    "        return '', ''\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    return title, file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the first image from our page, name it 'test.jpg'. Make sure it was downloaded correctly and see what the function returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_image(### YOUR answer HERE ###)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download all of the page's images, using a loop. \n",
    "\n",
    "First, create a folder named 'boys' in the current directory. You can do it right here in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir boys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While downloading, fill in the blanks to correctly create a dictionary called `images_data` which will hold the title of the image, its file name, and the shirt's price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "your_answer_1 = ### YOUR CODE HERE ###\n",
    "your_answer_2 = ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "images_data = {'title': {},\n",
    "               your_answer_1: {},\n",
    "               'price': {}}\n",
    "\n",
    "f = IntProgress(min = 0, max = len(images[1:])) # instantiate a progress bar\n",
    "display(f) # display the bar\n",
    "\n",
    "for i in range(len(images[1:])):\n",
    "    title, file_name = download_image(image_files[i], image_titles[i], './boys/' + str(i) + '.jpg')\n",
    "    images_data['title'][i] = title\n",
    "    images_data['file_name'][i] = file_name\n",
    "    images_data[your_answer_2][i] = prices[i]\n",
    "    f.value += 1\n",
    "\n",
    "        \n",
    "ans['Q4'] = your_answer_1 \n",
    "ans['Q5'] = your_answer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that would prove useful later on is having a dataset which summarizes all we have gathered. That's what `images_data` is for. We're going to use `pandas` to make it a `DataFrame` we can easily read and write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "images_data_df = pd.DataFrame(images_data)\n",
    "images_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was fun, we got 48 images. But we're looking to get times ~200 than that, and the same amount of shirts images for girls. The following code was run to get all boys shirts images. You can run it to see that it's working or you can just skim it to see you get how all the different elements are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys_url = 'https://il.ebay.com/b/Boys-Short-Sleeve-Sleeve-Tops-T-Shirts-Sizes-4-Up/175521/bn_4278610?rt=nc&LH_ItemCondition=1000&LH_BIN=1&LH_PrefLoc=3&_pgn='\n",
    "max_pages = 400\n",
    "boys_items_data = {'title': {}, 'file_id': {}, 'price': {}}\n",
    "f = IntProgress(min = 0, max = max_pages)\n",
    "display(f)\n",
    "all_items_counter = 0\n",
    "\n",
    "for page_num in range(max_pages):\n",
    "    url = boys_url + str(page_num)\n",
    "    try:\n",
    "        r = requests.get(url, \"lxml\")\n",
    "    except:\n",
    "        print('Stopped at page: ' + page_num)\n",
    "        break\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    images = soup.find_all('img')[1:]\n",
    "    image_titles = [img['alt'] for img in images]\n",
    "    image_files_src = [img['src'] for img in images]\n",
    "    image_files_datasrc = [img.get('data-src', None) for img in images]\n",
    "    image_files = [src if datasrc is None else datasrc for src, datasrc in zip(image_files_src, image_files_datasrc)]\n",
    "    \n",
    "    price_elements = soup.find_all('span', class_ = 's-item__price')\n",
    "    prices = [parse_price(price_e) for price_e in price_elements]\n",
    "    try:\n",
    "        assert len(prices) == len(images)\n",
    "    except:\n",
    "        print('Found unequal number of prices in page_num % d' % page_num)\n",
    "        prices = [None] * len(images)\n",
    "        \n",
    "    for i in range(len(images)):\n",
    "        title, file_name = download_image(image_files[i], image_titles[i], './boys/' + str(all_items_counter + i) + '.jpg')\n",
    "        boys_items_data['title'][all_items_counter + i] = title\n",
    "        boys_items_data['file_id'][all_items_counter + i] = all_items_counter + i\n",
    "        boys_items_data['price'][all_items_counter + i] = prices[i]\n",
    "    all_items_counter += len(images)\n",
    "    f.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you'll get all boys and girls images quicker, using the images that were downloaded for you. You should be able to do this only once.\n",
    "\n",
    "First download the compressed file from a remote server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.tau.ac.il/~saharon/DScourse/ebay_boys_girls_shirts.tar.gz\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open(\"ebay_boys_girls_shirts.tar\", \"wb\") as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next decompress the file in the datasets folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"ebay_boys_girls_shirts.tar\") as tar:\n",
    "    tar.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have in your datasets folder all ~33K boys and girls shirts images. See that you can read the four CSVs holding the metadata for the train and test sets of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'ebay_boys_girls_shirts/'\n",
    "boys_train_df = pd.read_csv(folder + 'boys_train.csv')\n",
    "girls_train_df = pd.read_csv(folder + 'girls_train.csv')\n",
    "boys_test_df = pd.read_csv(folder + 'boys_test.csv')\n",
    "girls_test_df = pd.read_csv(folder + 'girls_test.csv')\n",
    "print('N boys train images: %d' % boys_train_df.shape[0])\n",
    "print('N girls train images: %d' % girls_train_df.shape[0])\n",
    "print('N boys test images: %d' % boys_test_df.shape[0])\n",
    "print('N girls test images: %d' % girls_test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished!\n",
    "now you know how to scrape like a pro! <br>\n",
    "please hand in the csv through moodle so we could know that too...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 7:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
