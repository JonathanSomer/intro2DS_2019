{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7: logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions\n",
    "\n",
    "our course will be using an automatic grading system. <br>\n",
    "after each question there will appear a code block with some prepared code to add your answer to a dictionary that will be sent to the course server for grading. <br>\n",
    "please do not edit any code other than in placeholders marked `#### your code here ####` <br>\n",
    "__don't forget to run the code block after you write your answer.__\n",
    "\n",
    "\n",
    "you can add code blocks wherever you want in order to interact with datasets and play with your own code. <br>\n",
    "in the next code block plase fill in your id number and email account in the appropriate placees. <br>\n",
    "and __don't forget to run the block!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['HW'] = 'HW7'\n",
    "ans['id_number'] = 307923383"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1)\n",
    "Assume we fit a logistic regression and got a fit $\\hat{\\beta}$. Assume $\\hat{\\beta_1} = \\hat{\\beta_2} = 1$. Now we want to predict $\\widehat{P(y=1|x)}$ at two points $x_0$ and $\\tilde{x}_0$ that are equal in all coordinates except the first two. Assume $x_{01}=100, \\tilde{x}_{01} = 101$ and $x_{02}=0, \\tilde{x}_{02} = -1.$ Denote the resulting predictions at $x_0,\\tilde{x}_0$ by $\\hat{p}_0$ and $\\tilde{p}_0$ respectively. Which of the following holds: \n",
    "\n",
    "1. $\\hat{p}_0$ > $\\tilde{p}_0$ \n",
    "2. $\\hat{p}_0$ < $\\tilde{p}_0$ \n",
    "3. $\\hat{p}_0$ = $\\tilde{p}_0$ \n",
    "4. Not enough information to know \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q1'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2)\n",
    "For the same setup, now assume we have three points $x_0,u_0,v_0$ that are identical in all but the first coordinate, where we have $x_{01} = 0, u_{01}=1, v_{01}=2.$ Denote the resulting predictions by $\\hat{p}_0,\\hat{q}_0,\\hat{r}_0$ respectively. Which of the following holds: \n",
    "\n",
    "1. $\\frac{log(\\hat{p}_0)}{log(\\hat{q}_0)}= \\frac{log(\\hat{q}_0)}{log(\\hat{r}_0)}$\n",
    "<br>\n",
    "<br>\n",
    "2. $\\frac{\\hat{p}_0}{1-\\hat{p}_0}=\\frac{\\hat{q}_0}{1-\\hat{q}_0}=\\frac{\\hat{r}_0}{1-\\hat{r}_0}$\n",
    "<br>\n",
    "<br>\n",
    "3. $\\frac{\\hat{p}_0}{\\hat{q}_0}= \\frac{\\hat{q}_0}{\\hat{r}_0}$\n",
    "<br>\n",
    "<br>\n",
    "4. $\\frac{\\frac{\\hat{p}_0}{1-\\hat{p}_0}}{\\frac{\\hat{q}_0}{1-\\hat{q}_0}} = \\frac{\\frac{\\hat{q}_0}{1-\\hat{q}_0}}{\\frac{\\hat{r}_0}{1-\\hat{r}_0}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q2'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3)\n",
    "In the logistic regression we ran in class on the *saheart* data, we got a significant negative coefficient $\\hat{beta}_{fam}$ for the binary variable *famshit_absent* which has the obvious meaning: it is $1$ if there is no family history and $0$ if there is. Assume we replace this variable with a variable *famhist_present = 1 - famhist_absent* which is $1$ if there is family history and $0$ otherwise. The rest of the data (including the training-test split) is unchanged. What would happen to the estimated coefficients if we run exactly the same logistic regression on this new dataset? You can base your answer on calculations, or just try it out...<br>\n",
    "\n",
    "1. Only the intercept and $\\hat{\\beta}_{fam}$ would change, but we cannot say exactly how\n",
    "2. The intercept would change and $\\hat{\\beta}_{fam}$ would switch sign but have the same absolute value\n",
    "3. All coefficients would change, we cannot say exactly how\n",
    "4. Only $\\hat{\\beta}_{fam}$ would change, it would switch sign from negative to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q3'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.496069\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    chd   No. Observations:                  369\n",
      "Model:                          Logit   Df Residuals:                      359\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sun, 19 May 2019   Pseudo R-squ.:                  0.2335\n",
      "Time:                        16:00:26   Log-Likelihood:                -183.05\n",
      "converged:                       True   LL-Null:                       -238.81\n",
      "                                        LLR p-value:                 7.171e-20\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -4.5748      1.468     -3.117      0.002      -7.451      -1.698\n",
      "sbp                0.0010      0.007      0.158      0.875      -0.012       0.014\n",
      "tobacco            0.1156      0.033      3.536      0.000       0.052       0.180\n",
      "ldl                0.1976      0.073      2.703      0.007       0.054       0.341\n",
      "adiposity          0.0198      0.033      0.602      0.547      -0.045       0.084\n",
      "typea              0.0288      0.014      2.065      0.039       0.001       0.056\n",
      "obesity           -0.0491      0.048     -1.017      0.309      -0.144       0.046\n",
      "alcohol           -0.0013      0.005     -0.263      0.793      -0.011       0.008\n",
      "age                0.0473      0.014      3.397      0.001       0.020       0.075\n",
      "famhist_Absent    -0.8983      0.260     -3.461      0.001      -1.407      -0.390\n",
      "==================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "saheart = pd.read_table(\"http://statweb.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.data\", header = 0,sep=',',index_col=0)\n",
    "saheart.head()\n",
    "\n",
    "saheart_X=pd.get_dummies(saheart.iloc[:,:9]).iloc[:,:9]\n",
    "saheart_y=saheart.iloc[:,9]\n",
    "\n",
    "n = saheart_X.shape[0]\n",
    "tr_size = math.floor(0.8*n)\n",
    "te_size = n-tr_size\n",
    "tr_ind = random.sample(range(n),tr_size)\n",
    "Xtr = saheart_X.iloc[tr_ind,:]\n",
    "Xte = saheart_X.drop(saheart_X.index[tr_ind])\n",
    "Ytr = saheart_y.iloc[tr_ind]\n",
    "Yte = saheart_y.drop(saheart_y.index[tr_ind])\n",
    "\n",
    "# Xtr.famhist_Absent = 1-Xtr.famhist_Absent\n",
    "#print(Xtr.head())\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "logit_model=sm.Logit(Ytr, sm.add_constant(Xtr))\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           sbp  tobacco    ldl  adiposity  typea  obesity  alcohol  age  \\\n",
      "row.names                                                                 \n",
      "278        174     2.02   6.57      31.90     50    28.75    11.83   64   \n",
      "389        148     4.50  10.49      33.27     50    25.92     2.06   53   \n",
      "204        128     0.73   3.97      23.52     54    23.81    19.20   64   \n",
      "354        176     1.20   8.28      36.16     42    27.81    11.60   58   \n",
      "15         112     9.65   2.29      17.20     54    23.53     0.68   53   \n",
      "\n",
      "           famhist_Absent  \n",
      "row.names                  \n",
      "278                     1  \n",
      "389                     0  \n",
      "204                     0  \n",
      "354                     1  \n",
      "15                      1  \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.496069\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    chd   No. Observations:                  369\n",
      "Model:                          Logit   Df Residuals:                      359\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sun, 19 May 2019   Pseudo R-squ.:                  0.2335\n",
      "Time:                        16:00:27   Log-Likelihood:                -183.05\n",
      "converged:                       True   LL-Null:                       -238.81\n",
      "                                        LLR p-value:                 7.171e-20\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             -5.4731      1.458     -3.755      0.000      -8.330      -2.616\n",
      "sbp                0.0010      0.007      0.158      0.875      -0.012       0.014\n",
      "tobacco            0.1156      0.033      3.536      0.000       0.052       0.180\n",
      "ldl                0.1976      0.073      2.703      0.007       0.054       0.341\n",
      "adiposity          0.0198      0.033      0.602      0.547      -0.045       0.084\n",
      "typea              0.0288      0.014      2.065      0.039       0.001       0.056\n",
      "obesity           -0.0491      0.048     -1.017      0.309      -0.144       0.046\n",
      "alcohol           -0.0013      0.005     -0.263      0.793      -0.011       0.008\n",
      "age                0.0473      0.014      3.397      0.001       0.020       0.075\n",
      "famhist_Absent     0.8983      0.260      3.461      0.001       0.390       1.407\n",
      "==================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathansomer/.virtualenvs/ml/lib/python3.6/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "Xtr.famhist_Absent = 1-Xtr.famhist_Absent\n",
    "print(Xtr.head())\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "logit_model=sm.Logit(Ytr, sm.add_constant(Xtr))\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4)\n",
    "Based on the definitions from class, which of the following identities does not hold? \n",
    "\n",
    "1. $TP+FN=P$\n",
    "2. $TP+FP=\\hat{P}$\n",
    "3. $TPR+FPR=1$\n",
    "4. $TP\\leq min(P,\\hat{P})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q4'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5)\n",
    "Assume we have two competing models, $mod_1$ and $mod_2$, for the same 2-class classification problem, evlauated on the same test set. \n",
    "Using the definitions we gave in class, with subscript for model number (so $Recall_1$ is the recall of $mod_1$, etc.), which of the following guarantees that $mod_1$ has higher accuracy on the test set?\n",
    "\n",
    "1. $Precision_1 > Precision_2$ and $\\hat{P}_1 < \\hat{P}_2$ \n",
    "2. $TP_1>TP_2$ and $FN_1 < FN_2$\n",
    "3. $Recall_1 > Recall_2$ and  $\\hat{P}_1 > \\hat{P}_2$\n",
    "4. $Recall_1 > Recall_2$ and $FPR_1 < FPR_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = [#(TP) + #(TN)] / (TN + FN + TP + FP)\n",
    "\n",
    "1. \n",
    "Precision = TP / P_hat\n",
    "\n",
    "Precision1 > Precision2 ---> TP1 / P_hat1 > TP2 / P_hat2\n",
    "\n",
    "It could be the case that TP1 == TP2 in this case. We have more positive predictions in model1 but same amount of TP. In mod1 we have transferred negative samples to P_hat (or else would see more TP). So this is a less accurate model\n",
    "\n",
    "2. TP1 > TP2 and FN1 < FN2\n",
    "\n",
    "\n",
    "P = TP + FN\n",
    "N = TN + FP\n",
    "\n",
    "\n",
    "[#(TP1) + #(TN1)] / (N+P)\n",
    "\n",
    "[#(TP2) + #(TN2)] / (N+P)\n",
    "\n",
    "\n",
    "3. Recall1 > Recall2 and P1 > P2\n",
    "\n",
    "Recall1 = TP1 / P\n",
    "Recall2 = TP2 / P\n",
    "\n",
    "\n",
    "TP1 / P > TP2 / P ---> TP1 > TP2\n",
    "\n",
    "TP1 + FP1 > TP2 + FP2\n",
    "\n",
    "[#(TP1) + #(TN1)] / (N+P)\n",
    "\n",
    "[#(TP2) + #(TN2)] / (N+P)\n",
    "\n",
    "\n",
    "4. Recall1 > Recall2 and  FPR1 < FPR2\n",
    "\n",
    "TP1 > TP2\n",
    "\n",
    "\n",
    "FPR1 = FP1 / N\n",
    "\n",
    "FP1 / N < FP2 / N\n",
    "\n",
    "FP1 < FP2\n",
    "1-TN1 < 1-TN2\n",
    "TN1 >TN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q5'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6)\n",
    "I have a classification model $mod_1$ and I apply it to a test set, measure AUC and get $AUC_1=0.6$. I also construct two additional models: $mod_2=2mod_1$ and $mod_3 = 1-mod_1$ (that is, every prediction goes through the transformation), and measure their AUCs on the same test set. Which of the following holds: \n",
    "\n",
    "1. $AUC_1=AUC_2>AUC_3.$\n",
    "2. $AUC_1>AUC_2>AUC_3.$\n",
    "3. $AUC_1>AUC_2$, don't know about $AUC_3.$\n",
    "4. $AUC_1=AUC_3$, don't know about $AUC_2.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2mod1 means we double all probabilities. This scaling retains order so doesn't affect the AUC.\n",
    "\n",
    "\n",
    "\n",
    "The model outputs a probability. 1-p gives us the complement. \n",
    "So if the probability that a chosen positive will be ranked higher than a negative was 60% we now have 0.4 AUC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC interpretation: the probability that a randomlly chosen positive sample will have a higher score than a randomlly chosen negative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['Q6'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finish!\n",
    "\n",
    "to submit your HW please run this last code block and follow the instructions. <BR>\n",
    "this code will create a CSV file in the current directory on the azure notebooks project <br>\n",
    "please download it and submit it through moodle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_ans = pd.DataFrame.from_dict(ans, orient='index')\n",
    "if df_ans.shape[0] == 8:\n",
    "    df_ans.to_csv('{}_{}.csv'.format(ans['HW'],str(ans['id_number'])))\n",
    "    print(\"OK!\")\n",
    "else:\n",
    "    print(\"seems like you missed a question, make sure you have run all the code blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
